{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math  1376: Programming for Data Science\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 06: A prologue to machine learning\n",
    "---\n",
    "\n",
    "> Before machine learning, there was statistics. \n",
    "\n",
    "In this notebook, we briefly review linear regression to help set the stage and mindset for the rest of this module. \n",
    "\n",
    "Linear regression is a commonly studied topic in statistics and computational science. \n",
    "The concept of linear regression is also at the core of many machine learning algorithms, which makes it a great place to start our studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "\n",
    "This notebook is adapted from https://github.com/justmarkham/DAT4/blob/master/notebooks/08_linear_regression.ipynb, and that notebook was adapted from Chapter 3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Why are we exploring linear regression?\n",
    "- It is widely used\n",
    "- It runs fast\n",
    "- It is easy to use (not a lot of tuning required)\n",
    "- It is highly interpretable\n",
    "- It is the basis for many other methods (e.g., we see how artificial neurons are related to linear regression in the next lecture notebook)\n",
    "\n",
    "\n",
    "## Libraries\n",
    "\n",
    "We use [scikit-learn](http://scikit-learn.org/stable/) to perform linear regression. \n",
    "\n",
    "- We learn a bit more about scikit-learn in this module, but you could devote an entire course to this library (and still only scratch the surface of what it has to offer).\n",
    "\n",
    "- It is a good module for you to focus most of your energy/attention as you learn machine learning/data science since it provides a lot of the functionality for machine learning that you would in general need to use.\n",
    "\n",
    "- We do a targeted dive into classifiers available within scikit-learn in the part (b) lecture notebook that is provided as part of the extra course materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "---\n",
    "\n",
    "- Be able to apply linear regression models to data\n",
    "\n",
    "- Be able to interpret linear regression model coefficients\n",
    "\n",
    "- Be able to apply linear regression to polynomial models of data\n",
    "\n",
    "- Be able to apply multiple linear regression to multiple features\n",
    "\n",
    "- Apply dummy variables to categorical variables in order to perform linear regression and interpret results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook contents <a id='Contents'>\n",
    "\n",
    "\n",
    "* <a href='#advertising'>An advertisement for linear regression</a>\n",
    "\n",
    "* <a href='#activity-polynomial'>Activity: Linear *Polynomial* Regression</a>\n",
    "    \n",
    "* <a href='#activity-categories'>Activity:  Handling Categorical Predictors with Categories</a> \n",
    "    \n",
    "* <a href='#activity-summary'>Activity: Summary</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An advertisement for linear regression <a id='#advertising'></a>\n",
    "\n",
    "<span style='background:rgba(255,255,0, 0.25); color:black'> Run the code cell below and click the \"play\" button to see the first recorded lecture associated with this notebook.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Running this cell with embed the short recorded lecture associated with this part of the notebook\n",
    "# 2. Press on the \"play\" button to start the video.\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('1O8V3Ep6p_c', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into a DataFrame\n",
    "data = pd.read_csv('Advertising.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  # To get a \"big picture\" view of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()  # just the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the **features**?\n",
    "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- Radio: advertising dollars spent on Radio\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "What is the **response**?\n",
    "- Sales: sales of a single product in a given market (in thousands of widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the DataFrame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 200 **observations**, and thus 200 **markets** in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "fig, axs = plt.subplots(1, 3, sharey=True)\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "data.plot(kind='scatter', x='TV', y='Sales', ax=axs[0], figsize=(16, 8))\n",
    "data.plot(kind='scatter', x='Radio', y='Sales', ax=axs[1])\n",
    "data.plot(kind='scatter', x='Newspaper', y='Sales', ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions About the Advertising Data\n",
    "\n",
    "Let's pretend you work for the company that manufactures and markets this widget. The company might ask you the following: On the basis of this data, how should we spend our advertising money in the future to maximize profit?\n",
    "\n",
    "This general question might lead you to more specific questions:\n",
    "1. Is there a relationship between ads and sales?\n",
    "2. How strong is that relationship?\n",
    "3. Which ad types contribute to sales?\n",
    "4. What is the effect of each ad type of sales?\n",
    "5. Given ad spending in a particular market, can sales be predicted?\n",
    "\n",
    "We will explore these questions below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Simple linear regression is an approach for predicting a **quantitative response** using a **single feature** (or \"predictor\" or \"input variable\"). It takes the following form:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x$\n",
    "\n",
    "What does each term represent?\n",
    "- $y$ is the response\n",
    "- $x$ is the feature\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for x\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are called the **model coefficients**. To create your model, you must \"learn\" the values of these coefficients. And once we've learned these coefficients, we can use the model to predict sales!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating (\"Learning\") Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"06_estimating_coefficients.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What elements are present in the diagram?\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction of those calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"06_slope_intercept.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate the model coefficients for sales as a function of TV advertising data using linear regression. \n",
    "\n",
    "Check out the code documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature vector x and response vector y\n",
    "feature_cols = ['TV']\n",
    "x = data[feature_cols]\n",
    "y = data.Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow the usual sklearn pattern: (1) import, (2) instantiate, (3) fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(x, y)\n",
    "\n",
    "# print intercept and coefficients\n",
    "print(lm.intercept_) \n",
    "print(lm.coef_)  # Shows up as an array because there are usually many features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Model Coefficients\n",
    "\n",
    "How do we interpret the TV coefficient ($\\beta_1$)?\n",
    "- A \"unit\" increase in TV ad spending is **associated with** a 0.047537 \"unit\" increase in Sales.\n",
    "- Or more clearly: An additional $1,000 spent on TV ads is **associated with** an increase in sales of 47.537 widgets.\n",
    "\n",
    "Note that if an increase in TV ad spending was associated with a **decrease** in sales, $\\beta_1$ would be **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model for Prediction\n",
    "\n",
    "Let's say that there was a new market where the TV advertising spend was **$50,000**. What would we predict for the Sales in that market?\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n",
    "$$y = 7.032594 + 0.047537 \\times 50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually calculate the prediction\n",
    "7.032594 + 0.047537*50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we would predict Sales of **9,409 widgets** in that market.\n",
    "\n",
    "Of course, we can also use the linear model to make the prediction! \n",
    "\n",
    "***In general, it is expecting that there are multiple features in the model (as we later see), so we need to formulate the input as either a 2-D array or as a DataFrame.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict_array = np.array([[50]])  # Notice the double brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.predict(x_predict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is probably the better way to do it since it aligns\n",
    "# with the whole idea of using DataFrames to begin with.\n",
    "x_predict_df = pd.DataFrame({'TV': [50]})\n",
    "x_predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions on a new value\n",
    "lm.predict(x_predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Least Squares Line\n",
    "\n",
    "Let's make predictions for the **smallest and largest observed values of x**, and then use the predicted values to plot the least squares line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with the minimum and maximum values of TV\n",
    "x_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\n",
    "x_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for those x values and store them\n",
    "linear_preds = lm.predict(x_new)\n",
    "linear_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, plot the observed data\n",
    "data.plot(kind='scatter', x='TV', y='Sales')\n",
    "\n",
    "# then, plot the least squares line\n",
    "plt.plot(x_new, linear_preds, c='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well Does the Model Fit the data?\n",
    "\n",
    "The most common way to evaluate the overall fit of a linear model is by the **R-squared** value. R-squared is the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model, or the reduction in error over the **null model**. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)\n",
    "\n",
    "R-squared is between 0 and 1, and higher is better because it means that more variance is explained by the model. Here's an example of what R-squared \"looks like\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"06_r_squared.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the **blue line** explains some of the variance in the data (R-squared=0.54), the **green line** explains more of the variance (R-squared=0.64), and the **red line** fits the training data even further (R-squared=0.66). (Does the red line look like it's overfitting?)\n",
    "\n",
    "Let's calculate the R-squared value for our simple linear model using the score method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the R-squared value for the model\n",
    "lm.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that a \"good\" R-squared value? It's hard to say. The threshold for a good R-squared value depends widely on the domain. Therefore, it's most useful as a tool for **comparing different models**.\n",
    "\n",
    "Let's try some higher-order polynomials models in the following activity and compare it to the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n",
    "\n",
    "## <span style='background:rgba(0,255,255, 0.5); color:black'>Activity: Linear *Polynomial* Regression</span> <a id='activity-polynomial'/></a>\n",
    "\n",
    "The trick to adapt linear regression to nonlinear relationships between variables is to transform the feature data.\n",
    "The basic idea is to create a linear model that looks like this:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\cdots\n",
    "$$\n",
    "where the $x_1, x_2, x_3,$ and so on just involved functions applied to the single-dimensional input $x$.\n",
    "That is, we let $x_n = f_n(x)$, where $f_n()$ is some function that transforms the features.\n",
    "\n",
    "If we are working with polynomials so that $f_n(x) = x^n$ for some $n$, then a polynomial regression problem takes the form of:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\cdots\n",
    "$$\n",
    "Notice that this is *still a linear model*—the linearity refers to the fact that the coefficients $\\beta_n$ never multiply or divide each other.\n",
    "What we have effectively done is taken the one-dimensional $x$ values and projected them into a higher dimension. \n",
    "Then, a linear fit can be applied to try and account for more complicated relationships between $x$ and $y$.\n",
    "\n",
    "This is related to the idea of applying kernels to features to transform the feature space into a new space where relationships between features and response variables are more easily understood or explained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  I will show you how to transform the data by hand to fit a quadratic.<span style='background:rgba(255,255,0, 0.25); color:black'> Add comments to each of the code cells below to explain the individual lines of code in each cell (you do not need to add comments to the lines I have already provided comments for). </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp  # So we can make a copy of x without changing x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = cp.deepcopy(x)  # Now create a copy of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2.insert(1, \"TV^2\", x['TV']**2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = LinearRegression()\n",
    "lm2.fit(x_2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm2.intercept_)\n",
    "print(lm2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.score(x_2,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Well, the value is *slightly* better for the quadratic. What does this look like? <span style='background:rgba(255,255,0, 0.25); color:black'> Add comments to each of the code cells below to explain the individual lines of code in each cell (you do not need to add comments to the lines I have already provided comments for). </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_2 = pd.DataFrame({'TV': np.linspace(data.TV.min(), data.TV.max(), 20),\n",
    "                        'TV^2': np.linspace(data.TV.min(), data.TV.max(), 20)**2})\n",
    "x_new_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_preds = lm2.predict(x_new_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='scatter', x='TV', y='Sales')  # first, plot the observed data\n",
    "\n",
    "plt.plot(x_new, linear_preds, c='red', linewidth=2)\n",
    "plt.plot(x_new_2['TV'], quadratic_preds, c='k', linewidth=2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style='background:rgba(255,255,0, 0.25); color:black'> Create cells below to repeat the above process for a cubic approximation. Then, try something silly like a 6th or 10th order polynomial approximation. What do you think is the best model and why? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features. This is called **multiple linear regression**:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n",
    "\n",
    "Each $x$ represents a different feature, and each feature has its own coefficient. In this case:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper$\n",
    "\n",
    "We estimate these coefficients below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "lm_multiple = LinearRegression()\n",
    "lm_multiple.fit(X, y)\n",
    "\n",
    "# print intercept and coefficients\n",
    "print(lm_multiple.intercept_)\n",
    "print(lm_multiple.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret these coefficients? For a given amount of Radio and Newspaper ad spending, an **increase of $1000 in TV ad spending** is associated with an **increase in Sales of 45.765 widgets**. This interprets the first coefficient. How would you interpret the other coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "How should we decide **which features to include** in a linear model to balance simplicity with a goodness of fit? Here's one idea:\n",
    "- Try different models, and only keep the features in the model that result in larger coefficients.\n",
    "- Check whether the R-squared value goes up when you add new features.\n",
    "\n",
    "What are the **drawbacks** to this approach?\n",
    "- Linear models rely upon a lot of **assumptions** (such as the features being independent), and if those assumptions are violated (which they usually are), R-squared and other statistical values used to assess goodness of fit or importance of a coefficient are less reliable.\n",
    "- R-squared is susceptible to **overfitting**, and thus there is no guarantee that a model with a high R-squared value will generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-squared will always increase as you add more features to the model**, even if they are unrelated to the response. Thus, selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "There is alternative to R-squared called **adjusted R-squared** that penalizes model complexity (to control for overfitting), but it generally [under-penalizes complexity](http://scott.fortmann-roe.com/docs/MeasuringError.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the R-squared value for our the multiple linear regression model above\n",
    "# calculate the R-squared\n",
    "lm_multiple.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So is there a better approach to feature selection? **Cross-validation.** It provides a more reliable estimate of out-of-sample error. It is therefore a better way to choose which of your models will best **generalize** to out-of-sample data. \n",
    "\n",
    "There is extensive functionality for cross-validation in scikit-learn, including automated methods for searching different sets of parameters and different models. Importantly, cross-validation can be applied to any model, whereas the methods described above only apply to linear models.\n",
    "\n",
    "We will talk more about cross-validation in a future lecture, but it is a topic that is best investigated in depth in a more advanced course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n",
    "\n",
    "## <span style='background:rgba(0,255,255, 0.5); color:black'>Activity:  Handling Categorical Predictors with Categories</span> <a id='activity-categories'/></a>\n",
    "\n",
    "<span style='background:rgba(255,255,0, 0.25); color:black'> Read through this section carefully, and look for the highlighted questions/problems that you need to answer. </span>\n",
    "\n",
    "Up to now, all of our predictors have been numeric. What if one of our predictors was categorical?\n",
    "\n",
    "Let's create a new feature called **Size**, and randomly assign observations to be **small or large** defining the size of the market:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set a seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "# create a Series of booleans in which roughly half are True\n",
    "nums = np.random.rand(len(data))\n",
    "mask_large = nums > 0.5\n",
    "\n",
    "# initially set Size to small, then change roughly half to be large\n",
    "data['Size'] = 'small'\n",
    "data.loc[mask_large, 'Size'] = 'large'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style='background:rgba(255,255,0, 0.25); color:black'> What is the map method in the code cell below doing?</span>\n",
    "\n",
    "For scikit-learn, we need to represent all data **numerically**. If the feature only has two categories, we can simply create a **dummy variable** that represents the categories as a binary value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Series called IsLarge\n",
    "data['IsLarge'] = data.Size.map({'small':0, 'large':1})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redo the multiple linear regression and include the **IsLarge** predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate, fit\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print coefficients\n",
    "for i, j in zip(feature_cols, lm.coef_):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the **IsLarge coefficient**? For a given amount of TV/Radio/Newspaper ad spending, being a large market is associated with an average **increase** in Sales of 57.42 widgets (as compared to a Small market, which is called the **baseline level**).\n",
    "\n",
    "What if we had reversed the 0/1 coding and created the feature 'IsSmall' instead? The coefficient would be the same, except it would be **negative instead of positive**. As such, your choice of category for the baseline does not matter, all that changes is your **interpretation** of the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorical Predictors with More than Two Categories\n",
    "\n",
    "* <span style='background:rgba(255,255,0, 0.25); color:black'> Print out and comment on what the `mask_suburban` and `mask_urban` variables are and how they are being used.</span>\n",
    "\n",
    "Let's create a new feature called **Area**, and randomly assign observations to be **rural, suburban, or urban**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "\n",
    "# assign roughly one third of observations to each group\n",
    "nums = np.random.rand(len(data))\n",
    "mask_suburban = (nums > 0.33) & (nums < 0.66)\n",
    "mask_urban = nums > 0.66\n",
    "data['Area'] = 'rural'\n",
    "data.loc[mask_suburban, 'Area'] = 'suburban'\n",
    "data.loc[mask_urban, 'Area'] = 'urban'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to represent Area numerically, but we can't simply code it as 0=rural, 1=suburban, 2=urban because that would imply an **ordered relationship** between suburban and urban (and thus urban is somehow \"twice\" the suburban category).\n",
    "\n",
    "Instead, we create **another dummy variable**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three dummy variables using get_dummies, then exclude the first dummy column\n",
    "area_dummies = pd.get_dummies(data.Area, prefix='Area').iloc[:, 1:]\n",
    "\n",
    "# concatenate the dummy variable columns onto the original DataFrame (axis=0 means rows, axis=1 means columns)\n",
    "data = pd.concat([data, area_dummies], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we interpret the coding:\n",
    "- **rural** is coded as Area_suburban=0 and Area_urban=0\n",
    "- **suburban** is coded as Area_suburban=1 and Area_urban=0\n",
    "- **urban** is coded as Area_suburban=0 and Area_urban=1\n",
    "\n",
    "Why do we only need **two dummy variables, not three?** Because two dummies captures all of the information about the Area feature, and implicitly defines rural as the baseline level. (In general, if you have a categorical feature with k levels, you create k-1 dummy variables.)\n",
    "\n",
    "If this is confusing, think about why we only needed one dummy variable for Size (IsLarge), not two dummy variables (IsSmall and IsLarge).\n",
    "\n",
    "Let's include the two new dummy variables in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge', 'Area_suburban', 'Area_urban']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate, fit\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print coefficients\n",
    "for i, j in zip(feature_cols, lm.coef_):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the coefficients?\n",
    "- Holding all other variables fixed, being a **suburban** area is associated with an average **decrease** in Sales of 106.56 widgets (as compared to the baseline level, which is rural).\n",
    "- Being an **urban** area is associated with an average **increase** in Sales of 268.13 widgets (as compared to rural).\n",
    "\n",
    "**A final note about dummy encoding:** If you have categories that can be ranked (i.e., strongly disagree, disagree, neutral, agree, strongly agree), you can potentially use a single dummy variable and represent the categories numerically (such as 1, 2, 3, 4, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style='background:rgba(255,255,0, 0.25); color:black'> In code and markdown cells below (you should add more), repeat the above analysis assuming there are four new categories to the data that you need to consider in the analysis. Call these four new categories **ET**, **CT**, **MT** and **PT** to indicate the typical four timezones most people reside in within the United States. You get to choose how to randomly assign these categories to the data. You may want to look up the general population distribution in the USA to estimate how to do this.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Didn't We Cover?\n",
    "\n",
    "- Detecting collinearity\n",
    "- Diagnosing model fit\n",
    "- Transforming predictors to fit non-linear relationships\n",
    "- Interaction terms\n",
    "- Assumptions of linear regression\n",
    "- And so much more!\n",
    "\n",
    "You could certainly go very deep into linear regression, and learn how to apply it really, really well. It's an excellent way to **start your modeling process** when working a regression problem. However, it is limited by the fact that it can only make good predictions if there is a **linear relationship** between the features and the response, which is why more complex methods (with higher variance and lower bias) will often outperform linear regression.\n",
    "\n",
    "Therefore, we want you to understand linear regression conceptually, understand its strengths and weaknesses, be familiar with the terminology, and know how to apply it. However, we also want to spend time on many other machine learning models, which is why we aren't going deeper here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- To go much more in-depth on linear regression, read Chapter 3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), from which this lesson was adapted. Alternatively, watch the [related videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) or read this [quick reference guide](http://www.dataschool.io/applying-and-interpreting-linear-regression/) to the key points in that chapter.\n",
    "- This [introduction to linear regression](http://people.duke.edu/~rnau/regintro.htm) is much more detailed and mathematically thorough, and includes lots of good advice.\n",
    "- This is a relatively quick post on the [assumptions of linear regression](http://pareonline.net/getvn.asp?n=2&v=8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n",
    "\n",
    "## <span style='background:rgba(0,255,255, 0.5); color:black'>Activity: Summary</span> <a id='activity-summary'/>\n",
    "\n",
    "Summarize some of the key takeaways/points from this notebook in a list below and prepare a few code examples related to these takeaways/points in the code cells below. You need to have at least one example for each of your summary points and you need at least three summary points.\n",
    "\n",
    "In this notebook, we have seen the following:\n",
    "\n",
    "- [Your summary point 1 goes here]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Your summary point 2 goes here]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Your summary point 3 goes here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href='#Contents'>Click here to return to Notebook Contents</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
