{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math  1376: Programming for Data Science\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #We will use numpy in this lecture\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 06: The beginnings of Machine Learning\n",
    "\n",
    "In this lecture, we begin exploring how to use Python to perform the basics of machine learning. \n",
    "We are primarily going to touch on material covered in the initial chapters out of Sebastian Raschka's book [Python Machine Learning (3rd edition)](https://github.com/rasbt/python-machine-learning-book-3rd-edition). \n",
    "\n",
    "\n",
    "Specifically, we will selectively sample and build upon material from the following chapters: \n",
    "\n",
    "- Chapter 2: Training Machine Learning Algorithms for Classification ***(This notebook's focus)***\n",
    "- Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-Learn\n",
    "- Chapter 4: Building Good Training Datasets â€“ Data Preprocessing\n",
    "- Chapter 5: Compressing Data via Dimensionality Reduction\n",
    "- Chapter 6: Learning Best Practices for Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "In the spirit of making arbitrary and capricious decisions, I declare that everything beyond these chapters is *advanced* and left for topics in a future course or self-study.\n",
    "After going through this notebook, the assignment associated with it, and the part (b) lecture notebook, you should have a solid enough foundation to go through all the materials related to the first six chapters of Raschka's book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives (and an important note)\n",
    "---\n",
    "\n",
    "First, an important note: machine learning concepts will often appear harder than they are. \n",
    "Try to not let this discourage you.\n",
    "If details elude you, just focus on the bigger picture about *why* we are doing something.\n",
    "The *how* will come with time.\n",
    "\n",
    "By the end of this notebook, you should be able to\n",
    "* Explain what supervised learning means.\n",
    "* Implement and study the capabilities of artificial neurons as classifiers on various types of labeled data.\n",
    "* Explain the role of both training data and hyperparameters in building classifiers. \n",
    "* Understand the importance of linear or \"almost\" linear separability on the performance of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook contents <a id='Contents'>\n",
    "\n",
    "\n",
    "- <a href='#perceptron'>Part (a)(i): Our first classifier: the perceptron</a>\n",
    "\n",
    "  - <a href='#activity-intuition'>Activity: Build your intuition</a>\n",
    "<br><br>\n",
    "\n",
    "- <a href='#adaline'>Part (a)(ii): ADALINE = ADAptive LInear NEuron</a>\n",
    "\n",
    "    - <a href='#activity-no-gap'>Activity: A perceptron with linearly separable data and no gap</a>\n",
    "<br><br>\n",
    "- <a href='#activity-summary'>Activity: Summary</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)(i): Our first classifier: the perceptron <a id='perceptron'></a>\n",
    "\n",
    "---\n",
    "\n",
    "### A list of some useful source material for this section \n",
    "\n",
    "- https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Artificial_neuron\n",
    "\n",
    "    \n",
    "<span style='background:rgba(255,255,0, 0.25); color:black'> Run the code cell below and click the \"play\" button to see the first recorded lecture associated with this notebook.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Running this cell with embed the short recorded lecture associated with this part of the notebook\n",
    "# 2. Press on the \"play\" button to start the video.\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('r_uINJ8tDaQ', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The concept\n",
    "---\n",
    "\n",
    "The perceptron was developed more than half a century ago based on a neuron model. \n",
    "When you hear of neural networks or deep learning, you are *basically hearing* about something that connects many things that are *like* perceptrons (there are other types of neuron models) together. \n",
    "So, perceptrons are a good place to start our initial meandering into the field of machine leaning.\n",
    "\n",
    "From Wikipedia (emphasis my own):\n",
    "\n",
    "> In machine learning, the perceptron is an algorithm for **supervised learning** of **binary classifiers**. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
    "\n",
    "What does **supervised learning** mean? Simply put, it means we learn a function that maps inputs to outputs based on *training data* containing example input-output pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematics\n",
    "---\n",
    "\n",
    "Mathematically, suppose that $\\mathbf{x}\\in\\mathbb{R}^m$ denotes the $m$ features present in the data, then a linear binary classifier defined on $\\mathbf{x}$ is represented as the function \n",
    "$$\n",
    "\\large    f(\\mathbf{w}\\cdot\\mathbf{x} + b) = \\begin{cases}\n",
    "                        1, & \\mathbf{w}\\cdot\\mathbf{x} + b>0, \\\\\n",
    "                        -1, & \\text{else}.\n",
    "                    \\end{cases}\n",
    "$$\n",
    "Here, $\\mathbf{w}$ denotes a $m$-dimensional vector of weights and $b$ is the bias. Both of these must be *learned* from the training data. \n",
    "In the evaluation of $f(\\mathbf{w}\\cdot\\mathbf{x} + b)$, we must compute $\\mathbf{w}\\cdot\\mathbf{x}=\\sum_{i=1}^m w_ix_i$, which denotes the standard dot product, and can be computed using the `numpy.dot` function that takes as arguments two arrays.\n",
    "\n",
    "\n",
    "We use the perceptron to make binary decisions, i.e., whether or not data that possess $m$ identifying features should belong to one set or its complement (which are represented by the outputs 1 and -1, respectively). \n",
    "\n",
    "<img src=\"neuron_and_perceptron.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about the *bias* in various source materials\n",
    "---\n",
    "\n",
    "As shown in the figure above (adapted from some of the [source material](https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb)), it is quite typical to refer to the bias as the weight $w_0$ and consider the weight vector as $(m+1)$-dimensional instead of $m$-dimensional. \n",
    "\n",
    "Keep this in mind when reviewing other sources. \n",
    "\n",
    "We will *not* follow that convention here because the formulas involving the learning of the bias are *different* than the formulas involved with learning the weights $w_1,w_2,\\ldots, w_m$. By treating these separately, we avoid any conceptual (and mathematical) confusion regarding the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to a line of best fit\n",
    "---\n",
    "\n",
    "Recall the standard regression problem common in statistics of finding a line of best fit (see the prologue notebook to this module).\n",
    "The idea is to take some given data $\\{(x_i,y_i)\\}_{i=1}^N$ where the input data $x$ is 1-dimensional and the output data $y$ is also 1-dimensional and try to fit a line to it. \n",
    "In other words, we *assume* (usually after inspection of a scatter plot of the data) that a line is a good description of the trend of the data so that our best prediction of $y$ given some value of $x$ is given by $y=f(x)=ax+b$.\n",
    "\n",
    "We can rephrase the regression problem as trying to *learn* the values of $a$ (which is analogous to the weight vector) and $b$ (which describes a type of bias) from the *training data* $\\{(x_i,y_i)\\}_{i=1}^N$.\n",
    "\n",
    "There are many ways to solve the regression problem (e.g., using least squares techniques to determine $a$ and $b$), to analyze the *goodness of fit* of the line, and to study whether any of the underlying assumptions in the formulation/solution of the problem are violated. \n",
    "\n",
    "The difference for the perceptron is that the output $y=f(\\mathbf{w}\\cdot\\mathbf{x} + b)$ is either 1 or -1 (not a continuous variable), so we must consider some different algorithmic approaches for solving this problem (i.e., for learning the weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should a perceptron work?\n",
    "---\n",
    "\n",
    "We need the data set to be ***linearly separable***. The idea that linearity plays some role is perhaps not surprising given that the analogy we used involved a line of best fit.\n",
    "\n",
    "What does linearly separable mean? It means that we can construct a line (for 2-dimensional input data sets), a plane (for 3-dimensional input data sets), or hyperplanes (for 4- and higher-dimensional input data sets) that *separates* the input data space into two parts associated with the output classes (i.e., the values of $y$). \n",
    "\n",
    "<img src=\"linearly_separable_example.png\" title='Linearly separable example' width=30%>\n",
    "\n",
    "See the following for some more background and illustrative examples:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Linear_separability\n",
    "\n",
    "- https://www.commonlounge.com/discussion/6caf49570d9c4d0789afbc544b32cdbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A training algorithm for learning weights and bias\n",
    "---\n",
    "\n",
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Terminology and notation:*** <span>\n",
    "\n",
    "\n",
    "- Let $\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^N=\\{(x_1^{(i)},x_2^{(i)},\\ldots,x_m^{(i)},y^{(i)})\\}_{i=1}^N$ denote the $N$ training data we use to learn the weights $\\mathbf{w}\\in\\mathbb{R}^m$ and bias $b$. \n",
    "\n",
    "- For simplicity, we assume that $y^{(i)}$ values are already scalarized to be 1's and -1's depending on the output classification.\n",
    "\n",
    "- We conceptualize the *optimal* weights and bias as the vector $\\mathbf{w}^\\text{opt}$ and scalar $b^\\text{opt}$ such that $y^{(i)}=f(\\mathbf{w}^\\text{opt}\\cdot\\mathbf{x}^{(i)}+b^\\text{opt})$ for all $1\\leq i\\leq N$.\n",
    "\n",
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Connection to root-finding problems:*** <span>\n",
    "\n",
    "If the optimal weights and bias are found, then for all $1\\leq i\\leq N$, we have \n",
    "\n",
    "$$y^{(i)}-f(\\mathbf{w}^\\text{opt}\\cdot\\mathbf{x}^{(i)}+b^\\text{opt})=0.$$ \n",
    "\n",
    "We wrote it like this for a reason: it looks like the optimal weights and bias solve a set of root-finding problems!\n",
    "\n",
    "Recall that most root-finding algorithms require an initial guess of the weights and bias. \n",
    "This gives us the first step of the training algorithm.\n",
    "\n",
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Algorithm:*** <span>\n",
    "\n",
    "\n",
    "**Step 1 (Initialize):** Initialize the weight vector $\\mathbf{w}^{(0)}$ and bias $b^{(0)}$ (zero or small random numbers are standard options).\n",
    "\n",
    "\n",
    "**Step 2 (Update? But, how?)**\n",
    "\n",
    "The goal is to determine some systematic way to incrementally update the weights and bias so that they approach their optimal values. But, how?\n",
    "\n",
    "For simplicity in developing the idea, assume $m=1$ (i.e., there is only one feature to the data so $\\mathbf{x}=x$ and the $i$th input sample is denoted by $x^{(i)}$), then we seek a weight $w^\\text{opt}$ and bias $b^\\text{opt}$ such that $w^\\text{opt}x^{(i)}+b^\\text{opt}>0$ whenever $y^{(i)}=1$ and is less than or equal to zero whenever $y^{(i)}=-1$.\n",
    "\n",
    "Having updated the weight and bias based on the first $i-1$ data points, suppose we evaluate $f(x^{(i)})$ with these values and observe one of the following scenarios:\n",
    "\n",
    "<img src=\"misclassification.png\" title=\"Misclassification implies we need to update weights and bias\" width=50%>\n",
    "\n",
    "We need to update the weights and bias to shift the function $f$ either to the left or right to try to remove this misclassification. \n",
    "Increasing the net input $wx+b$ will shift the function to the left while decreasing the net input $wx+b$ will shift the function to the right. \n",
    "If this is confusing or seems backwards, try plotting $f(x)=x$ and then plot $f(x+1)=x+1$ and $f(x-1)=x-1$. \n",
    "\n",
    "Suppose we need to shift the function to the left.\n",
    "This implies we observe that $y^{(i)}-f(w^{(i-1)}x^{(i)}+b^{(i)})=2$.\n",
    "\n",
    "Suppose instead we need to shift the function to the right.\n",
    "This implies we observe that $y^{(i)}-f(w^{(i-1)}x^{(i)}+b^{(i)})=-2$. \n",
    "    \n",
    "In either case, the misclassification can be used to determine the *sign* of the updates to the weights and bias. \n",
    "\n",
    "The updating is in fact given as follows\n",
    "\n",
    "**Step 2 (Updating):** For $i=1,2,\\ldots, N$, do the following:\n",
    " \n",
    " - Let $\\mathbf{w}^{(i-1)}$ and $b^{(i-1)}$ denote the weights and bias used to predict the $i$th output data $y^{(i)}$. \n",
    "\n",
    " - Let $0<r\\leq 1$ denote a ***learning rate***. \n",
    "\n",
    " - Compute misclassification error: $$y^{(i)}-f(\\mathbf{w}^{(i-1)}\\cdot\\mathbf{x}^{(i)}+b^{(i-1)})$$\n",
    "\n",
    " - Update the weights \n",
    " \n",
    " $$\\mathbf{w}^{(i)} = \\mathbf{w}^{(i-1)} + r\\left[y^{(i)}-f(\\mathbf{w}^{(i-1)}\\cdot\\mathbf{x}^{(i)}+b^{(i)})\\right]\\mathbf{x}^{(i)}$$\n",
    " \n",
    " and the bias \n",
    " \n",
    " $$b^{(i)} = b^{(i-1)} + r\\left[y^{(i)}-f(\\mathbf{w}^{(i-1)}\\cdot\\mathbf{x}^{(i)}+b^{(i)})\\right].$$\n",
    " \n",
    "**Step 3 (Rinse and Repeat):** We repeat Step 2 until either the misclassifications disappear (or fall below some tolerance) or a maximum number of iterations are reached. The number of times we loop through all the training data to update the weights are called the ***epochs*** of the algorithm. If all misclassification errors disappear, then call the final weights and bias \"optimal.\" If there are still some misclassification errors, then call these final weights and bias \"near optimal.\" \n",
    "\n",
    "*If the data are not linearly separable, then we can never get the misclassification errors to completely disappear. We should also not be using a perceptron in that case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs, learning rate, say what now?\n",
    "---\n",
    "\n",
    "The learning rate and epochs discussed in steps 2 and 3 above are called ***hyperparameters***. A hyperparameter refers to some variable whose value requires specification in order to run the algorithm. \n",
    "\n",
    "What values should we choose for hyperparameters? The answer is: *it depends*. \n",
    "\n",
    "Depends on what? The answer is: *on the problem*. \n",
    "\n",
    "Oh, well that just clears up everything doesn't it?! Problem dependent choices for hyperparameters are necessary to get optimal performance of the algorithm. How is that helpful? The answer is: *it's not*.\n",
    "\n",
    "You are welcome! A not helpful answer is just what you wanted, isn't it? Well, we can attempt to automate the choice of hyperparameters that are *nearly* optimal based on the data we have to train the model by using **cross-validation** (but, as mentioned before, this is a topic best left for a more advanced course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the performance of the algorithm over epochs\n",
    "---\n",
    "\n",
    "We first visualize the performance with computations below using 1- and 2-dimensional feature spaces involving ***linearly separable*** data, i.e., data for which the algorithm *should* converge if the *epochs* are either sufficiently high (relative to the size of the learning rate and quality of the initial guess).\n",
    "\n",
    "We will first interrogate the performance on linearly separable data sets that have nice \"fat\" gaps between the two data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(x, w, b):  # This is what performs the classification\n",
    "    return np.where(np.dot(x,w)+b > 0, 1, -1)  # Slick way of using np.where, huh? What is this doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_perceptron_weights(x, y, w, b, r, num_epochs): \n",
    "    \n",
    "    n = x.shape[0]\n",
    "    \n",
    "    for j in range(num_epochs):\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            error = y[i] - activation_function(x[i], w, b)\n",
    "            w += r*error*x[i]\n",
    "            b += r*error\n",
    "            \n",
    "    return (w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1d_training_data(num_training=100, gap=0.2): \n",
    "    # A function for simulating training data with x in [0,1]\n",
    "    pt1 = 0.4  # if x < pt1, then y=-1\n",
    "    pt2 = pt1+gap  # if x> pt2, then y=1\n",
    "    # Probability of a training point being 1 or -1 depends on lengths of [0,pt1] and [pt2,1]\n",
    "    prob1 = pt1/(pt1+(1-pt2))\n",
    "    prob2 = (1-pt2)/(pt1+(1-pt2))\n",
    "    pseudo_data = np.random.uniform(low=0, high=1, size=num_training)\n",
    "    x_data = np.zeros(num_training)\n",
    "    for i in range(num_training):\n",
    "        if pseudo_data[i]<prob1:\n",
    "            x_data[i] = np.random.uniform(low=0, high=pt1)\n",
    "        else:\n",
    "            x_data[i] = np.random.uniform(low=pt2, high=1)\n",
    "    y_data = np.where(x_data > pt2, 1, -1)\n",
    "    return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_data, y_data) = create_1d_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_updates_1d_perceptron(x, y, w, b, r, num_epochs):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    plt.axhline(0, linewidth=1, linestyle='--', c='k')  # plot typical x-axis\n",
    "    \n",
    "    ax.set_ylim([-1.25, 2])\n",
    "    ax.scatter(x, y, s=50, c='r', label='training data')\n",
    "    \n",
    "    x_plot = np.linspace(np.min(x)-0.1*(np.max(x)-np.min(x)), \n",
    "                         np.max(x)+0.1*(np.max(x)-np.min(x)), \n",
    "                         101)\n",
    "    y_plot = activation_function(x_plot, w, b)\n",
    "    ax.plot(x_plot, y_plot, 'b:', linewidth=2, label='initial perceptron')\n",
    "\n",
    "    ####### Learn the optimal weights and bias ################\n",
    "    w, b = learn_perceptron_weights(x, y, w, b, r, num_epochs)\n",
    "    ###########################################################\n",
    "    \n",
    "    y_plot = activation_function(x_plot, w, b)\n",
    "    ax.plot(x_plot, y_plot, 'k-', linewidth=2, label='learned perceptron')\n",
    "    ax.legend(fontsize=14)\n",
    "    ax.text(np.mean(x_plot), 1.1, r\"Learned $w$=%3.2f and $b$=%3.2f\" %(w,b),\n",
    "            horizontalalignment='center', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_updates_1d_perceptron, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w = widgets.FloatSlider(value=-5, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1), \n",
    "         r = widgets.FloatSlider(value=0.5, min=0.01, max=1, step=0.01), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the 2-d example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_training_data(num_training=100): \n",
    "    # Create training data with two features\n",
    "\n",
    "    # Now create a box for class -1.\n",
    "    # The first row is the lower-left point and \n",
    "    # second row is upper-right point for this box\n",
    "    box_1 = np.array([[0.5, 0], [1.5, 4]]) \n",
    "\n",
    "    # Similar idea for a box of class 1\n",
    "    box_2 = np.array([[2, 2], [4, 4]]) \n",
    "\n",
    "    # Will assume equal probability of training data being in either box\n",
    "    pseudo_data = np.random.uniform(low=0, high=1, size=num_training)\n",
    "    x_data = np.zeros((num_training,2))\n",
    "    y_data = np.zeros(num_training) \n",
    "    for i in range(num_training):\n",
    "        if pseudo_data[i]<0.5:\n",
    "            x_data[i,0] = np.random.uniform(low=box_1[0,0], high=box_1[1,0])\n",
    "            x_data[i,1] = np.random.uniform(low=box_1[0,1], high=box_1[1,1])\n",
    "            y_data[i] = -1\n",
    "        else:\n",
    "            x_data[i,0] = np.random.uniform(low=box_2[0,0], high=box_2[1,0])\n",
    "            x_data[i,1] = np.random.uniform(low=box_2[0,1], high=box_2[1,1])\n",
    "            y_data[i] = 1\n",
    "    return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_data, y_data) = create_2d_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def visualize_updates_2d_perceptron(x, y, w1, w2, b, r, num_epochs):\n",
    "    \n",
    "    w = np.array([w1, w2])\n",
    "    \n",
    "    x1_plot = np.linspace(np.min(x[:,0])-0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          np.max(x[:,0])+0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          51)\n",
    "    x2_plot = np.linspace(np.min(x[:,1])-0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          np.max(x[:,1])+0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          51)\n",
    "    x1_plot, x2_plot = np.meshgrid(x1_plot, x2_plot)\n",
    "    x_plot = np.vstack((x1_plot.flatten(), x2_plot.flatten())).T\n",
    "    y_plot = activation_function(x_plot, w, b)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "    \n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    axs[0].contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "    \n",
    "    idx_1 = np.where(y==1)[0]\n",
    "    idx_0 = np.where(y==-1)[0]\n",
    "    \n",
    "    axs[0].scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    axs[0].scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    axs[0].set_title('Initial perceptron', fontsize=14)\n",
    "    \n",
    "    ####### Learn the optimal weights and bias ################\n",
    "    w, b = learn_perceptron_weights(x, y, w, b, r, num_epochs)\n",
    "    ###########################################################\n",
    "    \n",
    "    y_plot = activation_function(x_plot, w, b)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "    \n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    axs[1].contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "    \n",
    "    axs[1].scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    axs[1].scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    \n",
    "    axs[1].set_title('Learned $w$=(%3.2f,%3.2f) and $b$=%3.2f' %(w[0], w[1], b), fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_updates_2d_perceptron, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w1 = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         w2 = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.FloatSlider(value=0.1, min=0.01, max=1, step=0.01), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n",
    "\n",
    "## <span style='background:rgba(0,255,255, 0.5); color:black'>Activity: Build your intuition</span><a id='activity-intuition'>\n",
    "---\n",
    "     \n",
    "The purpose of this activity is to build your intuition about the performance of the training algorithm for the perceptron. For each part, you should try the code on different sets of training data with different sized \"gaps\" in the classes to make sure that your statements/comments are actually correct. \n",
    "\n",
    "Feel free to add code/markdown cells below as you work through this activity.\n",
    "\n",
    "- For different training sets with different gaps, set the number of epochs sufficiently high so that the optimal weights and bias are learned. The optimal weights and bias are *not* unique. They depend upon the initial guesses and learning rates. \n",
    "\n",
    "    - For fixed initial guesses, comment on the sensitivity of the optimal weights and bias that are learned as the learning rate is varied.\n",
    "\n",
    "    - For a fixed learning rate, comment on the sensitivity of the optimal weights and bias that are learned as the initial guesses are varied.\n",
    "  \n",
    "- For different training sets with different gaps, fix initial guesses that are *not* optimal. Comment on how many epochs are required to obtain optimal weights and bias as the learning rate is varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"unreality\" of linear separability exposed by realistic data\n",
    "\n",
    "John Hollinger is a former ESPN analyst/writer who developed some advanced statistics to quantify efficiencies of players and teams in the NBA. He parlayed this into becoming the VP of basketball operations for the Memphis Grizzlies in 2012. If you have ever heard of PER (Player Efficiency Rating) when evaluating a player's chances of winning awards such as MVP, then you are at least somewhat familiar with the contributions of Mr. Hollinger. \n",
    "\n",
    "Below, we *crawl* through some online data courtesy of ESPN. By courtesy of ESPN what I actually mean is that according to the terms of service from Disney (the parent company of ESPN): \n",
    "\n",
    "> G. Informational and Entertainment Purposes. You understand that the Disney Products are for your personal, noncommercial use and are intended for informational and entertainment purposes only; the content available does not constitute legal, financial, professional, medical or healthcare advice or diagnosis and cannot be used for such purposes.\n",
    "\n",
    "Anyway, let's see how well team efficiency statistics in the regular season correspond to a predictor of *success* defined by a team making the playoffs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first orient ourselves on the types of data sets we will be using\n",
    "df_regular_season_temp = pd.read_html('http://www.espn.com/nba/hollinger/teamstats/_/year/2019')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postseason_temp = pd.read_html('http://www.espn.com/nba/hollinger/teamstats/_/year/2019/seasontype/3')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regular_season_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postseason_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Off_Eff = []\n",
    "Def_Eff = []\n",
    "\n",
    "y = []\n",
    "\n",
    "iter = 0\n",
    "for year in range(2009, 2020):\n",
    "    url = 'http://www.espn.com/nba/hollinger/teamstats/_/year/' + str(year)\n",
    "    df_regular_season = pd.read_html(url)[0]\n",
    "    Off_Eff.append(df_regular_season.loc[2:, 10].values)\n",
    "    Def_Eff.append(df_regular_season.loc[2:, 11].values)\n",
    "    \n",
    "    N = len(df_regular_season.loc[2:, 1].values)\n",
    "    \n",
    "    y.append(-np.ones(N))\n",
    "    url += '/seasontype/3'\n",
    "    df_playoffs = pd.read_html(url)[0]\n",
    "    \n",
    "    playoff_teams = df_playoffs.loc[2:,1].values\n",
    "    all_teams = df_regular_season.loc[2:,1].values\n",
    "    for i in range(16):\n",
    "        for j in range(N):\n",
    "            if playoff_teams[i] == all_teams[j]:\n",
    "                y[iter][j] = 1\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Off_Eff_All = np.concatenate(Off_Eff).astype('float')\n",
    "Def_Eff_All = np.concatenate(Def_Eff).astype('float')\n",
    "\n",
    "bball_features = np.vstack((Off_Eff_All.flatten(), Def_Eff_All.flatten())).T\n",
    "\n",
    "playoffs_All = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_playoffs = np.where(playoffs_All==1)[0]\n",
    "idx_no_playoffs = np.where(playoffs_All==-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(bball_features[idx_playoffs, 0], bball_features[idx_playoffs, 1], \n",
    "            s=20, c='r', marker='s')\n",
    "\n",
    "plt.scatter(bball_features[idx_no_playoffs,0], bball_features[idx_no_playoffs, 1], \n",
    "            s=20, c='b', marker='o')\n",
    "\n",
    "plt.xlabel('OFF EFF', fontsize=14)\n",
    "plt.ylabel('DEF EFF', fontsize=14)\n",
    "\n",
    "plt.title('Playoff Teams (red) and Non-Playoff Teams (blue)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, now what?\n",
    "\n",
    "The data above sure *looks* like it is reasonably separated into two distinct classes although there is a *bit* of overlap. \n",
    "\n",
    "However, it is ***not*** linearly separable, which will mean bad things for the convergence of the training algorithm for the perceptron. We can still *try* to train a perceptron on this data. It just may not work. (Okay, it is *guaranteed* to not work.)\n",
    "\n",
    "However, we still need to address one key point: how do we *train* the perceptron?! In other words, how do we learn the weights and bias required to define the line that separates the *feature space* of offensive and defensive efficiency into playoff vs non-playoff teams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to train a perceptron on the basketball data that are *almost* linearly separable (meaning, not linearly separable)\n",
    "\n",
    "- Well, convergence is *not* going to happen, but with enough epochs, the updates may just *settle down* towards some good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_updates_2d_perceptron, \n",
    "         x = widgets.fixed(bball_features),\n",
    "         y = widgets.fixed(playoffs_All),\n",
    "         w1 = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         w2 = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=-2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.FloatSlider(value=0.1, min=0.01, max=1, step=0.01), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some closing remarks on the perceptron and foreshadowing\n",
    "---\n",
    "\n",
    "Linear separability is a ***strict requirement*** for the perceptron. \n",
    "\n",
    "The algorithms for learning the weights and bias in a perceptron will not work unless data are linearly separable. \n",
    "\n",
    "This is more than just mildly annoying, it is essentially impractical in many cases where we observe an ***almost*** linear separability.\n",
    "\n",
    "<img src=\"almost_linearly_separable_example.png\" title='Almost separable' width=30%>\n",
    "\n",
    "There is something called a ***kernel trick*** that is very common in machine learning. \n",
    "\n",
    "The basic premise is this: Maybe there is a function (called the kernel) that can transform the data in such a way that this transformed data are linearly separable. Then, we simply carry out our usual analysis except on this transformed data. \n",
    "\n",
    "When can we do this? Well, if it looks like there is a way to separate the data in some nonlinear way, then there is hope that we can figure out some kernel that allows us to linearly separate the transformed data. \n",
    "\n",
    "<img src=\"not_linearly_separable_example.png\" title='Nonlinearly separable illustration (what is the kernel trick?)' width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)(ii): [ADALINE = ADAptive LInear NEuron](https://en.wikipedia.org/wiki/ADALINE) <a id='adaline'>\n",
    "---\n",
    "    \n",
    "<span style='background:rgba(255,255,0, 0.25); color:black'> Run the code cell below and click the \"play\" button to see the second recorded lecture associated with this notebook.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Running this cell with embed the short recorded lecture associated with this part of the notebook\n",
    "# 2. Press on the \"play\" button to start the video.\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('-ThqPPKgdvE', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Key points:*** <span>\n",
    "\n",
    "\n",
    "- When data are only *almost* linearly separable instead of absolutely linearly separable, we are going to have to live with some misclassifications (i.e., *predictions* given by $f(\\mathbf{w}^\\text{opt}\\cdot\\mathbf{x}^{(i)}+b^\\text{opt})$  that do not equal the reality of $y^{(i)}$ for some subset of $1\\leq i\\leq N$). \n",
    "\n",
    "- This means that what is *optimal* will in general mean that we minimize the misclassification percent rather than simply make it zero. This idea is explored more in our next neuron model ADALINE, so just be patient for now.\n",
    "\n",
    "- ADALINE is an alternative neuron model developed a few years after the perceptron was introduced. \n",
    "\n",
    "- Unlike the perceptron that learns the weights and bias based on the misclassification error being -2, 0, or 2 due to the use of a step-activation function acting on the *net input function* defined by $\\mathbf{w}\\cdot\\mathbf{x} + b$, the activation function in ADALINE is the identity function, which means we simply use the errors defined by $y^{(i)} - \\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)} + b\\right)$ to learn the weights and bias. \n",
    "\n",
    "- Once the weights and bias are learned, we still compute $f\\left(\\mathbf{w}\\cdot\\mathbf{x}+b\\right)$ to classify any particular feature vector $\\mathbf{x}$. The function $f$ from before is now considered a ***quantizer*** (as opposed to the activation function, which is now the identity function) that maps the net value into the class. \n",
    "\n",
    "We summarize this in the figure below. \n",
    "\n",
    "<img src=\"perceptron_vs_ADALINE.png\" width=50%>\n",
    "\n",
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Summary of main difference:*** <span>\n",
    "\n",
    "> While $y$ is just -1's and 1's, the output of $\\left(\\mathbf{w}\\cdot\\mathbf{x} + b\\right)$ is *continuous*, so the *error* defined by $y^{(i)} - \\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)} + b\\right)$ does not just take on discrete values but instead provides a number corresponding to *how much* we are right/wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this change the way in which we learn the weights?\n",
    "\n",
    "We first write the **sum of square errors (SSE)** from the $N$ training data as the *cost function* (sometimes called an *objective* function)\n",
    "$$\n",
    " \\large   J(\\mathbf{w},b) = \\frac{1}{2}\\sum_{i=1}^N \\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right]^2. \n",
    "$$\n",
    "\n",
    "Observe that $J(\\mathbf{w},b)$ is a *quadratic* function over the space of weights and bias. The *objective* is to learn the weights and bias that *minimize* the *cost* function. This is conceptualized in the picture below.\n",
    "\n",
    "<img src=\"cost-functional.png\" title=\"The cost function is like a big bowl\" width=50%>\n",
    "\n",
    "### There is a unique minimum?\n",
    "\n",
    "***Correct!*** Since the cost function is a quadratic function, it has a unique global minimum even if the data are not linearly separable! (This is the part where you say \"ooohhh\".)\n",
    "\n",
    "This means that you can always train an ADALINE...at least theoretically..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximating the minimum with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "First, a quick review of some useful facts from calculus and how we use them (you only need to understand how to apply these, so if you are not familiar with calculus, don't worry). \n",
    "\n",
    "- The [gradient](https://en.wikipedia.org/wiki/Gradient) of a scalar valued multivariate function evaluated at some input is a *vector* that points in the direction of greatest increase of function values. This means that the *negative* of the gradient at some input will point in the direction of greatest *decrease* of function values.  \n",
    "\n",
    "   - Why is this important to us? We want to minimize a function, so we will be using the *negative gradient*.\n",
    "\n",
    "   - What do we need to be aware of? The *length* of the gradient vector (also called its magnitude) can complicate the decision making process in terms of how far we should step in the direction of the negative gradient.\n",
    "\n",
    "       - If the length of the gradient vector is too large, then we may step *too far* so that we end up *increasing* the value of the function instead of *decreasing* the value! This is just plain counterproductive. \n",
    "\n",
    "       - If the length of the gradient is too small or we just choose to make too small of steps, then it may take *way too long* to have this method converge. \n",
    "\n",
    "The image below may prove useful to refer as it summarizes why we move in the direction of the negative gradient.\n",
    "\n",
    "<img src=\"cost-functional-gradient.png\" title=\"The negative direction of gradient is what we want!\" width=\"50%\" />\n",
    "\n",
    "Below, we demonstrate the gradient descent algorithm for a 1-D function that shares similarities with the \"parabolic bowl\" shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the quadratic f(x)=20*x^2 + 2, with derivative 40*x and minimum at x=0\n",
    "\n",
    "def visualize_gradient_descent(x_old, r, num_iter):\n",
    "    f = lambda x: 10*x**2 + 2\n",
    "    df = lambda x: 20*x\n",
    "    \n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = f(x)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.plot(x, y, 'b', linewidth=2, alpha=0.25)\n",
    "    \n",
    "    ax.scatter(x_old, f(x_old), s=50, c='r')\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        x_new = x_old - r*df(x_old)\n",
    "        ax.scatter(x_new, f(x_new), s=50, c='r')\n",
    "        ax.arrow(x=x_old, y=f(x_old), dx=(x_new-x_old), dy=(f(x_new)-f(x_old)))\n",
    "        x_old = x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_gradient_descent, \n",
    "         x_old = widgets.FloatSlider(value=2, min=0.1, max=3, step=0.1),\n",
    "         r = widgets.FloatText(value=0.025, step=0.01),\n",
    "         num_iter = widgets.IntSlider(value=1, min=1, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some gradient details for the cost functional (this is not as bad as it may look)\n",
    "---\n",
    "\n",
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Key points:*** <span>\n",
    "\n",
    "- The gradient is a vector of partial derivatives where the $j$th component of the vector is the partial derivative of the function with respect to the $j$th input variable. \n",
    "\n",
    "- If there are $m$ features in our data, then the gradient is an $(m+1)$-dimensional vector because there are $m$ weights and 1 bias, i.e., $J(\\mathbf{w}, b)$ is a function of $(m+1)$ variables.\n",
    "\n",
    "- The partial derivatives of $J(\\mathbf{w},b)$ are denoted by\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial w_j} = -\\sum_{i=1}^N \\Big(\\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right]x_j^{(i)}\\Big)\n",
    "$$\n",
    "     and\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial b} = -\\sum_{i=1}^N \\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right].\n",
    "$$\n",
    "     Then, the gradient of $J(\\mathbf{w},b)$, denoted by $\\nabla J(\\mathbf{w},b)$, is given by the $(m+1)$-dimensional vector\n",
    "$$\n",
    "\\large    \\nabla J(\\mathbf{w}, b) = \\left[\\begin{array}{c} \n",
    "                                    \\frac{\\partial J}{\\partial w_1} \\\\\n",
    "                                    \\frac{\\partial J}{\\partial w_2} \\\\\n",
    "                                    \\vdots \\\\\n",
    "                                    \\frac{\\partial J}{\\partial w_m} \\\\\n",
    "                                    \\frac{\\partial J}{\\partial b}\n",
    "                                    \\end{array}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling it all together\n",
    "---\n",
    "\n",
    "Taking all of the above together, we arrive at the rather simple approach to updating a current guess of weights and bias. \n",
    "\n",
    "\n",
    "- Given a ***learning rate***, denoted by $r$, determine the additive updates to the weights and bias using the following\n",
    "$$\n",
    "     \\left[\\begin{array}{c} \n",
    "                 \\Delta w_1 \\\\\n",
    "                 \\Delta w_2 \\\\\n",
    "                 \\vdots \\\\\n",
    "                  \\\\\n",
    "                 \\Delta w_m \\\\\n",
    "                 \\Delta b\n",
    "                 \\end{array}\\right] =  -r\\left[\\begin{array}{c} \n",
    "                                    \\frac{\\partial J}{\\partial w_1} \\\\\n",
    "                                    \\frac{\\partial J}{\\partial w_2} \\\\\n",
    "                                    \\vdots \\\\\n",
    "                                    \\frac{\\partial J}{\\partial w_m} \\\\\n",
    "                                    \\frac{\\partial J}{\\partial b}\n",
    "                                    \\end{array}\\right] = -r\\nabla J(\\mathbf{w}, b)\n",
    "$$\n",
    "Simply put, this means that for $1\\leq j\\leq m$, \n",
    "$$\n",
    "    \\Delta w_j = r\\sum_{i=1}^N \\Big(\\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right]x_j^{(i)}\\Big),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\Delta b = r\\sum_{i=1}^N \\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right].\n",
    "$$\n",
    "\n",
    "    In other words, the learning rate determines how far we move in the direction of the negative gradient (i.e., it dictates the step size). We will see just how important it is to choose this hyperparameter correctly.\n",
    "\n",
    "### What are the epochs now?\n",
    "\n",
    "Note that now we update all of the weights using all of the training data at once. An epoch describes how many times we compute the gradient and update the weights just as before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait one minute, how is this different than finding a linear function of best fit?\n",
    "\n",
    "It is basically the same thing. In fact, the gradient descent used in the ADELINE algorithm to learn the weights and bias will just converge (well, maybe) to what we would get if we just used [least squares](https://en.wikipedia.org/wiki/Linear_least_squares) (which is also related to standard [regression](https://en.wikipedia.org/wiki/Linear_regression) from statistics). \n",
    "\n",
    "Why did I say *maybe* about the convergence? Well, the gradient descent algorithm is iterative and can be a bit sensitive to initial conditions (i.e., initial guesses of weights and bias) and the learning rate $r$ as hinted at above. \n",
    "We explore this in the code cell below.\n",
    "\n",
    "Well gosh, if there are all these issues, then why don't we just use least squares to find the function $\\mathbf{w}\\cdot\\mathbf{x}+b$ instead of gradient descent?\n",
    "Least squares will just give the solution using a closed-form expression. No iterations necessary. \n",
    "\n",
    "This is a good question for you to research. You will find the answer pretty quickly if you read about the [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) method that is actually preferred in practice for big data sets.\n",
    "\n",
    "As you learn more about machine learning and computational science in general, it is a good idea for you to conceptualize a taxonomy of scenarios where you may prefer using one method to another. If there are at least two algorithms/approaches to solve any problem, there will always be problems where one method works better than another. It is good to know when and why this happens. This usually requires learning more about mathematics (linear algebra and numerical analysis), statistics (including some basic probability theory), and computer science. You do *not* need to master any or all of these, but it is best to develop a deep understanding in at least one of those areas while being effectively *literate/knowledgeable/fluent* in the language of the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we play!\n",
    "\n",
    "Back to the 1-D and 2-D examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizer(x, w, b):  # Really the same thing as the activation function from before\n",
    "    return np.where(np.dot(x,w)+b > 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_ADALINE_weights(x, y, w, b, r, num_epochs):\n",
    "    \n",
    "    for j in range(num_epochs):\n",
    "        errors = y - (np.dot(x,w)+b)\n",
    "        w += r*np.dot(x.T,errors)\n",
    "        b += r*np.sum(errors)\n",
    "    \n",
    "    return (w, b, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_updates_1d_ADALINE(x, y, w, b, r, num_epochs):\n",
    "    \n",
    "    x_plot = np.linspace(np.min(x)-0.1*(np.max(x)-np.min(x)), \n",
    "                         np.max(x)+0.1*(np.max(x)-np.min(x)), \n",
    "                         101)\n",
    "    y_plot = quantizer(x_plot,w,b)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    ax.plot(x_plot, y_plot, 'b:', linewidth=2, label='initial classification')\n",
    "\n",
    "    ax.set_ylim([-1.25, 1.75])\n",
    "    ax.scatter(x, y, s=50, c='r')\n",
    "\n",
    "    num_training_data = np.size(x)\n",
    "    \n",
    "    ####### Learn the optimal weights and bias ################\n",
    "    w, b, errors = learn_ADALINE_weights(x, y, w, b, r, num_epochs)\n",
    "    ###########################################################\n",
    "    \n",
    "    y_plot = quantizer(x_plot, w, b)\n",
    "    ax.plot(x_plot, y_plot, 'k', linewidth=2, label='learned classification')\n",
    "    \n",
    "    ax.plot(x_plot, w*x_plot+b, 'k-.', linewidth=1, label='net input')\n",
    "    \n",
    "    ax.legend(fontsize=12)\n",
    "    \n",
    "    ax.axhline(y=0)\n",
    "    ax.text(np.mean(x_plot), 1.1, r\"Learned $w$=%3.2f and $b$=%3.2f\" %(w,b),\n",
    "            horizontalalignment='center', fontsize=12)\n",
    "    \n",
    "    ax.set_title('$J(w,b)$=%3.2e' %(0.5*np.sum(errors**2)), fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_data, y_data) = create_1d_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_updates_1d_ADALINE, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w = widgets.FloatSlider(value=1.5, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1), \n",
    "         r = widgets.fixed(1e-2), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=1, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now remove the gap in the 1-D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_data, y_data) = create_1d_training_data(gap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_updates_1d_ADALINE, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w = widgets.FloatSlider(value=1.5, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1), \n",
    "         r = widgets.fixed(1e-2), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n",
    "\n",
    "## <span style='background:rgba(0,255,255, 0.5); color:black'>Activity: A perceptron with linearly separable data and no gap<a id='activity-no-gap'></a>\n",
    "---\n",
    "\n",
    "Create a code cell below to try learning the weights for a **perceptron** when the data are linearly separable but without a well-defined gap as above. Comment on what you see in a Markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_updates_2d_ADALINE(x, y, w1, w2, b, r, num_epochs):\n",
    "    w = np.array([w1, w2])\n",
    "    \n",
    "    x1_plot = np.linspace(np.min(x[:,0])-0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          np.max(x[:,0])+0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          51)\n",
    "    x2_plot = np.linspace(np.min(x[:,1])-0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          np.max(x[:,1])+0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          51)\n",
    "    x1_plot, x2_plot = np.meshgrid(x1_plot, x2_plot)\n",
    "    x_plot = np.vstack((x1_plot.flatten(), x2_plot.flatten())).T\n",
    "    y_plot = activation_function(x_plot, w, b)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "    \n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    axs[0].contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "    \n",
    "    idx_1 = np.where(y==1)[0]\n",
    "    idx_0 = np.where(y==-1)[0]\n",
    "    \n",
    "    axs[0].scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    axs[0].scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    axs[0].set_title('Initial ADALINE', fontsize=12)\n",
    "    \n",
    "    ####### Learn the optimal weights and bias ################\n",
    "    w, b, errors = learn_ADALINE_weights(x, y, w, b, r, num_epochs)\n",
    "    ###########################################################\n",
    "    \n",
    "    y_plot = activation_function(x_plot, w, b)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "    \n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    axs[1].contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "    \n",
    "    axs[1].scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    axs[1].scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    \n",
    "    axs[1].set_title('Learned $w$=(%3.2f,%3.2f) and $b$=%3.2f' %(w[0], w[1], b), fontsize=18)\n",
    "    \n",
    "    axs[1].set_title('$J(w,b)$=%3.2e' %(0.5*np.sum(errors**2)), fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "# Create training data\n",
    "num_training = 100\n",
    "bbox_1 = np.array([[0.5, 0], [1.5, 4]]) #first row is lower-left point and second row is upper-right point of class -1\n",
    "bbox_2 = np.array([[2, 2], [4, 4]]) # \" \" of class 1\n",
    "# Will assume equal probability of training data being in either box\n",
    "pseudo_data = np.random.uniform(low=0, high=1, size=num_training)\n",
    "x_data = np.zeros((num_training,2))\n",
    "y_data = np.zeros(num_training)\n",
    "for i in range(num_training):\n",
    "    if pseudo_data[i]<0.5:\n",
    "        x_data[i,0] = np.random.uniform(low=bbox_1[0,0], high=bbox_1[1,0])\n",
    "        x_data[i,1] = np.random.uniform(low=bbox_1[0,1], high=bbox_1[1,1])\n",
    "        y_data[i] = -1\n",
    "    else:\n",
    "        x_data[i,0] = np.random.uniform(low=bbox_2[0,0], high=bbox_2[1,0])\n",
    "        x_data[i,1] = np.random.uniform(low=bbox_2[0,1], high=bbox_2[1,1])\n",
    "        y_data[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(visualize_updates_2d_ADALINE, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w1 = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         w2 = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.fixed(1e-3), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=1, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting the basketball data with an ADALINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_updates_2d_ADALINE, \n",
    "         x = widgets.fixed(bball_features),\n",
    "         y = widgets.fixed(playoffs_All),\n",
    "         w1 = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         w2 = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=-2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.fixed(1e-7), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional important concepts\n",
    "---\n",
    "\n",
    "1. Many machine learning algorithms benefit from [***feature scaling***](https://en.wikipedia.org/wiki/Feature_scaling) (sometimes called *data normalization*). Gradient descent in particular benefits from feature scaling. What is feature scaling? It is a *pre-processing* of data. Perhaps the most common feature scaling method is ***standardization***. The basic idea is that by re-scaling the ranges of the individual features into comparable length scales, we can better assess how to weight the individual features based on their importance in explaining the variation in data. \n",
    "\n",
    "> Imagine we collect data on heights and weights of adults, and then try to predict some health outcome based on this data (e.g., whether or not they have arthritis, heart disease, or other conditions that we believe may be linked to an individual's size). If the height is in feet, then the range might be $[4, 7]$, and if the weight is in pounds, then the range might be $[70, 500]$. These disparate magnitudes and ranges of values will often negatively impact our machine learning algorithm. Standardization (also called Z-score normalization) will transform each feature so that its sample mean is zero and sample standard deviation is one. This will generally improve the performance of the machine learning algorithm. \n",
    "\n",
    "You can Google \"importance of standardization of data\" to get more information on the topic. We simply mention it is important here.\n",
    "\n",
    "2. Variants of gradient descent may be required for large data sets. While we are dealing with relatively small data sets for the purposes of learning how to do things, you should be aware of some important variants of algorithms that rely on gradient descent that are useful for \"big data.\" These include [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) and mini-batch gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A useful lesson in failure\n",
    "---\n",
    "\n",
    "It is sometimes useful to understand a *bad* example. \n",
    "I just happened to create the original version of this notebook mere days after the NFL \"Zoom\" Draft of 2020.\n",
    "I really tried to incorporate some data relevant to the draft into this notebook. \n",
    "Below, I describe my thought process and why I ultimately failed.\n",
    "\n",
    "At the NFL combine, players that may be drafted have various physical and mental attributes measured. \n",
    "For player $X$, let $\\mathbf{x}=(x_1,x_2,\\ldots,x_m)$ denote the $m$-attributes that are measured. \n",
    "Perhaps $x_1$ denotes height, $x_2$ denotes the weight, $x_3$ denotes the 40-yard dash time, $x_4$ denotes the number of times the player bench presses 225lbs, $x_5$ denotes how high the player can jump, and so on (maybe the last attribute $x_m$ denotes their score on the Wonderlic test, which seems to be important more for quarterbacks and offensive lineman although even that is debatable). \n",
    "\n",
    "\n",
    "A practical question is: can we analyze past combine data for each position and develop a perceptron to predict whether or not a player at a particular position will perform at a pro-bowl level?\n",
    "\n",
    "### The lesson learned\n",
    "---\n",
    "Why did I say this is a bad example? Well, I pulled *years* (5-10 years) of combine data from pro-football-reference.com at various skill positions (QB, RB, WR), and I could *not* see any approximate separability (even with a kernel trick) in the data. I *think* there are too many confounding factors in the data that impact player success beyond individual physical/mental features such as the quality of the team that drafts the player, how the player's talents fit into the style of the team, and of course injuries (and suspensions) that derail many careers. \n",
    "\n",
    "***So, what is the lesson?*** It is basically the same lesson you probably learned when computing lines of best fit, which is that not every data set should be modeled with a line! Data science follows the same rules as all of computational science and statistics in that no single method should be universally applied to every problem. This is just a silly thing to assume. Best to get this point deep into your mind as early as possible.\n",
    "    \n",
    "### A good question: what happens if there are multiple classes?\n",
    "\n",
    "There are plenty of prestigious accolades an NFL player may receive: Pro-Bowler, All-Pro, All-Decade, Hall of Fame, or even all of these. \n",
    "So, what happens in this case? \n",
    "Or, in the language of perceptons or ADALINE models, how do we deal with multiple output classes, i.e., when $y$ can taken on more than two values?\n",
    "\n",
    "We punt on that for the moment (sorry, couldn't resist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:rgba(0,255,255, 0.5); color:black'>A final (suggested) activity on a class of classifiers</span>\n",
    "\n",
    "- Use some code cells below to construct a `neuron` class along with `perceptron` and `ADALINE` sub-classes. Refer to code above and the source material (https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb) for help. \n",
    "\n",
    "  - You need to determine which common variables/functions should be attributes within the `neuron` class (e.g., the net input function and prediction/classification functions are the *same* for each neuron, and each one has an array of weights and a bias variable) and what should be unique to the sub-classes (e.g., the way in which we fit/learning of weights and bias). The source material can really help if you are stuck, but these are great things to think about (and even struggle a bit) before turning to the source material. \n",
    "\n",
    "\n",
    "- Instantiate some perceptron and ADALINE objects and apply them to data sets seen above. Provide visualizations and analysis of results using a mixture of code and Markdown cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n",
    "\n",
    "## <span style='background:rgba(0,255,255, 0.5); color:black'>Activity: Summary</span> <a id='activity-summary'/>\n",
    "\n",
    "Summarize some of the key takeaways/points from this notebook in a list below and prepare a few code examples related to these takeaways/points in the code cells below. You need to have at least one example for each of your summary points and you need at least three summary points.\n",
    "\n",
    "In this notebook, we have seen the following:\n",
    "\n",
    "- [Your summary point 1 goes here]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Your summary point 2 goes here]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Your summary point 3 goes here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href='#Contents'>Click here to return to Notebook Contents</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
