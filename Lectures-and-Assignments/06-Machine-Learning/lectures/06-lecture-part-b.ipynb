{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math  1376: Programming for Data Science\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #We will use numpy in this lecture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from matplotlib.patches import Polygon \n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d #This enables 3d plotting\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 06: The beginnings of Machine Learning\n",
    "\n",
    "In this lecture, we begin exploring how to use Python to perform the basics of machine learning. \n",
    "We are primarily going to touch on material covered in the initial chapters out of Sebastian Raschka's book [Python Machine Learning (3rd edition)](https://github.com/rasbt/python-machine-learning-book-3rd-edition). \n",
    "\n",
    "\n",
    "Specifically, we will selectively sample and build upon material from the following chapters: \n",
    "\n",
    "- Chapter 2: Training Machine Learning Algorithms for Classification\n",
    "- Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-Learn ***(This notebook's focus)***\n",
    "- Chapter 4: Building Good Training Datasets â€“ Data Preprocessing\n",
    "- Chapter 5: Compressing Data via Dimensionality Reduction\n",
    "- Chapter 6: Learning Best Practices for Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "In the spirit of making arbitrary and capricious decisions, I declare that everything beyond these chapters is *advanced* and left for topics in a future course or self-study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "---\n",
    "\n",
    "* Be able to implement classifiers from the scikit-learn library.\n",
    "* Explain the difference between training and test data and be able to split a dataset into training and test data subsets.  \n",
    "* Implement feature-scaling. \n",
    "* Explain what a support vector machine is, the margin, and how we avoid overfitting.\n",
    "* Implement the kernel trick for nonlinear problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook contents <a id='Contents'>\n",
    "\n",
    "\n",
    "* <a href='#tour'>Part (b)(i): A Tour of Machine Learning Classifiers and Data-Preprocessing Using [Scikit-Learn](https://scikit-learn.org/stable/)</a>\n",
    "\n",
    "* <a href='#svm'>Part (b)(ii): Support Vector Machines (SVMs)</a>\n",
    "\n",
    "* <a href='#kernel-trick'>Part (b)(iii): The kernel trick for nonlinear problems</a>    \n",
    "\n",
    "* <a href='#activity-summary'>Activity: Summary</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tour'> Part (b)(i): A Tour of Machine Learning Classifiers and Data-Preprocessing Using [Scikit-Learn](https://scikit-learn.org/stable/) </a>\n",
    "\n",
    "---\n",
    "\n",
    "### Some useful source material\n",
    "\n",
    "- https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch03/ch03.ipynb\n",
    "- https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch04/ch04.ipynb\n",
    "- https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch06/ch06.ipynb\n",
    "- https://scikit-learn.org/stable/ ***<----- !!!! Bookmark this !!!!***\n",
    "- https://scikit-learn.org/stable/supervised_learning.html#supervised-learning \n",
    "\n",
    "<span style='background:rgba(255,255,0, 0.25); color:black'> Run the code cell below and click the \"play\" button to see the first recorded lecture associated with this notebook.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Running this cell with embed the short recorded lecture associated with this part of the notebook\n",
    "# 2. Press on the \"play\" button to start the video.\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('Still-need-to-create', width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jumping right into the fire: comparing classifiers with scikit-learn\n",
    "---\n",
    "\n",
    "Below, we parse out the [comparing classifiers example](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) from scikit-learn. \n",
    "\n",
    "I have included some of my own comments to this example including questions about interesting attributes each of the classification objects possesses. \n",
    "\n",
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Goals for this example:*** <span>:\n",
    "\n",
    "1. Develop an overall sense of the variety of classifiers available in scikit-learn. \n",
    "<br><br>\n",
    "\n",
    "2. Observe the performance of these classifiers on datasets with different qualitative characteristics.\n",
    "\n",
    "    - This will also illustrate the variety of tools available for constructing artificial datasets to test classifiers or for applying new algorithms that we may develop on our own.\n",
    "<br><br>\n",
    "\n",
    "3. Understand how some of the available data preprocessing tools work. \n",
    "\n",
    "    - This includes both feature scaling and splitting datasets into train/test subsets to assess performance.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import a bunch of functions from scikit-learn\n",
    "#\n",
    "# Pay attention to which ones end with \"Classifier\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of names describing the various classifiers being compared - useful for annotating plots\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the various classifiers being compared\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data sets with different qualitative aspects to compare classifiers\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(27,9))\n",
    "h = .02  # step size in the mesh - this makes plots look \"nice\"\n",
    "\n",
    "i = 1 # Used to index plotting\n",
    "\n",
    "# Now iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # First preprocess datasets by splitting into training and test parts\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X) #FEATURE SCALING!!! BUT THIS IS IN THE WRONG PLACE!\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42) #WHAT IS THIS???\n",
    "\n",
    "    # Setup plotting variables\n",
    "    # I would have opted for x1_* and x2_* (or x0_* and x1_*) variable names\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train) # Learn the weights and bias\n",
    "        score = clf.score(X_test, y_test) # WHAT IS THIS???\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):  # WHAT IS THIS??? hasattr? Look it up - related to classes\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well that was...illuminating? Probably not...interesting? Maybe...\n",
    "---\n",
    "\n",
    "Okay, what was that exactly? It was just an overview. Scikit-learn has a lot of nice built-in classifiers and general machine learning tools built right into it. It is a bit of a rabbit hole, but we can *navigate* this rabbit hole with what we have learned so far. Along the way, we will learn about some of the best practices in data science. \n",
    "\n",
    "As a first step, let's first familiarize ourselves with some of the documentation for the data preprocessing tools used in the code above. \n",
    "\n",
    "### Feature scaling\n",
    "---\n",
    "\n",
    "We begin with the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) that was imported with this command\n",
    "`from sklearn.preprocessing import StandardScaler`\n",
    "\n",
    "In the [source](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/preprocessing/_data.py#L520) code (see the top-right area of the documentation page), you will see that `StandardScaler` is a class with a few useful attributes. It also inherits some methods from the base class such as [`fit_transform`](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/base.py#L544) which is a useful wrapper function for any data pre-processing sub-class. In fact, the only pre-processing done was the following\n",
    "\n",
    "> `X = StandardScaler().fit_transform(X)`\n",
    "\n",
    "This simply removes the mean from data and then divides by the standard deviation (i.e., performing data normalization), and returns the output, which we use to overwrite the previous `X` data.\n",
    "\n",
    "Below, we first take the `linearly_separable` dataset, perform standard scaling, and then compare it to the original dataset. We also do this with the \"moons\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The linearly separable dataset\n",
    "data_set_idx = 2\n",
    "X, _ = datasets[data_set_idx]\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(datasets[data_set_idx][0][:,0], datasets[data_set_idx][0][:,1], c='b')\n",
    "plt.scatter(X[:,0], X[:, 1], c='r')\n",
    "plt.axhline(0, linewidth=1, linestyle='--', c='k') #plot typical x-axis\n",
    "plt.axvline(0, linewidth=1, linestyle='--', c='k') #plot typical y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The moons data set\n",
    "data_set_idx = 0\n",
    "X, _ = datasets[data_set_idx]\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(datasets[data_set_idx][0][:,0], datasets[data_set_idx][0][:,1], c='b')\n",
    "plt.scatter(X[:,0], X[:, 1], c='r')\n",
    "plt.axhline(0, linewidth=1, linestyle='--', c='k') #plot typical x-axis\n",
    "plt.axvline(0, linewidth=1, linestyle='--', c='k') #plot typical y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did I say the feature scaling was in the wrong place?\n",
    "---\n",
    "\n",
    "Look at my comment in the code:\n",
    "\n",
    "> `X = StandardScaler().fit_transform(X) #FEATURE SCALING!!! BUT THIS IS IN THE WRONG PLACE!`\n",
    "\n",
    "***This should be after the training and testing subsets are formed.*** It should also be modified so that\n",
    "\n",
    "- The scaler is first *learned* (i.e., fitted) to the *training* data that it subsequently transforms. \n",
    "\n",
    "- The scaler is then *applied* to the *testing* data (i.e., we transform the testing data using the scaling learned from the training data). \n",
    "\n",
    "We discuss this more below.\n",
    "\n",
    "**Lesson?** Not every example you find, even in official documentation, is following the best practices of machine learning. You need to *think* about what you are running. Almost everything runs on the same \"garbage in, garbage out\" principle no matter how shiny that garbage may look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and testing subsets: what, how, and why?\n",
    "---\n",
    "\n",
    "Following the (incorrect placement of) scaling of the datasets, the datasets were then split into training and testing subsets using the following command:\n",
    "\n",
    "> `X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)`\n",
    "        \n",
    "Later in the code, when we iterate over each classifier, we see that the training and testing subsets are used in different ways. Specifically, we see that\n",
    "\n",
    "> `clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)`\n",
    "        \n",
    "The `clf` is the variable used for the current *classifier* in the loop, which clearly means that all classifiers contain methods named `fit` and `score`. \n",
    "Choosing the documentation for any particular classifier (e.g., the [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)), we can read about the `fit` and `score` methods.\n",
    "\n",
    "From the documentation, it is apparent that `fit` is a method that is called to \"build\" the classifier from the data passed to it. In other words, this is the method that ***learns*** whatever parameters are required by the particular classifier (such as weights and bias for our previous classifiers). \n",
    "\n",
    "Similarly, we see that the `score` method produces a mean score on how well the classifier built on training data correctly predicts the labels in the test data. If you dive into the source code a bit, then you will see that this is simply returning the percentage of correctly predicted labels from the test data (there is an optional keyword argument `sample_weight` that can allow you to penalize some incorrect predictions more than others). \n",
    "\n",
    "If we used all of the available data to build the classifier, then we could possibly do a better job of minimizing classification errors on this dataset. So, why do we split the data into training and testing subsets? Ostensibly, we expect that we are going to use these classifiers to predict what class some new data belongs to that we have not yet seen. The process of splitting datasets for which we already know the classifications into training and testing subsets allows us to develop a confidence in how the classifier should perform on data it has not yet seen. In other words, we try to avoid [***overfitting***](https://en.wikipedia.org/wiki/Overfitting) the model so that it explains the training data very well but fails to generalize as reliably to unseen data. We will dive into this a little deeper with regards to a concept known as regularization. \n",
    "\n",
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Summarizing key takeaways:*** <span>:\n",
    "\n",
    "1. What are training and testing subsets?\n",
    "\n",
    "   > Training sets are used to build the classifier (i.e., they are used in the ***learning*** of parameters). Testing subsets are used to assess the classifiers predictive ability.\n",
    "\n",
    "2. How do we split datasets into training and testing subsets?\n",
    "\n",
    "   > The [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) function has various keyword arguments that allow you to specify how you want to split the dataset into training and testing subsets. \n",
    " \n",
    "  ***Note if you are using or encounter code based on older versions of `sklearn`:*** This function used to be in the submodule `cross_validation` in older versions of `sklearn`, but that submodule has been [deprecated](https://en.wikipedia.org/wiki/Deprecation#Software_deprecation) with certain functionality like the `train_test_split` method being absorbed in the submodule `model_selection`.\n",
    "<br><br>  \n",
    "3. Why do we split datasets?\n",
    "\n",
    "   > To avoid overfitting the model so that it fails to perform well on unseen data.\n",
    "   \n",
    "Since *unseen* data will have to first be *scaled* before it can be classified, it should now be rather evident why we should *wait* to transform data until after the splitting into training and testing data.\n",
    "You can read more about why we want to do this here: https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split. \n",
    "I particularly like the part about ***data leakage***. \n",
    "It sounds just like a nasty side effect of a drug, and you should definitely stop what you are doing and consult your teacher at the first sign of data leakage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:rgba(0,255,255, 0.5); color:black'>**Suggested activity**</span>\n",
    "\n",
    "---\n",
    "\n",
    "- Copy/paste the code cell that loops through the classifiers and \"fix\" the way that the feature scaling is done as discussed above.\n",
    "<br><br>\n",
    "\n",
    "- Experiment with different `test_size` and `random_state` values in the construction of the training and testing subsets. \n",
    "<br><br>\n",
    "\n",
    "- Make all of this fancier using widgets that allow you to specify different values of `test_size` and `random_state` inputs.\n",
    "<br><br>\n",
    "\n",
    "- Comment on your findings in a Markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I know my train and test split was the best one? (Hint: Cross-validation)\n",
    "---\n",
    "\n",
    "Now *that* is a good question. \n",
    "\n",
    "You can *never* know that!\n",
    "\n",
    "Maybe the way you split the dataset ended up with training data that was really bad for learning the parameters in a model. Oh well, huh? I guess you just decide to not use that model then because of a poor choice of training data split from the original dataset. \n",
    "\n",
    "WRONG!\n",
    "\n",
    "[Cross-validation (CV)](https://scikit-learn.org/stable/modules/cross_validation.html) offers a solution. \n",
    "Of course, there are multiple types of CV methods.\n",
    "The basic idea of most CV methods is to split your dataset into many smaller chunks and then loop through the training/testing phases on these chunks of data. \n",
    "\n",
    "CV methods are useful (but can be expensive) for choosing the \"optimal\" model, but they do not actually produce the final model. \n",
    "What we typically do is then train the chosen model on *all* of the available data (e.g., read [this](https://machinelearningmastery.com/train-final-machine-learning-model/) for a discussion on training a final model). \n",
    "\n",
    "This is probably frustrating to read because we just discussed above how we want to avoid overfitting the model, which is why we do not train it on all of the data. \n",
    "\n",
    "Are we going in circles?\n",
    "\n",
    "Actually, we are in a great place to now discuss regularization methods that help avoid overfitting. Of course, regularization introduces a new hyperparameter (remember learning rates and epochs?). Fortunately, CV methods can also be used to determine \"optimal\" hyperparameters.  \n",
    "\n",
    "For the sake of clarity and succinctness, we limit the presentation of these ideas to support vector machines (SVM) for the remainder of this lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)(ii): Support Vector Machines (SVMs) <a id='svm'>\n",
    "---\n",
    "    \n",
    "<span style='background:rgba(255,255,0, 0.25); color:black'> Run the code cell below and click the \"play\" button to see the second recorded lecture associated with this notebook.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Running this cell with embed the short recorded lecture associated with this part of the notebook\n",
    "# 2. Press on the \"play\" button to start the video.\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('Still-need-to-create', width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [support vector machine (SVM)](https://en.wikipedia.org/wiki/Support-vector_machine) is an extension of the perceptron.\n",
    "Recall that the perceptron really only works on linearly separable data where it seeks to learn the parameters of the net input function $\\mathbf{w}\\cdot\\mathbf{x}+b$ such that the hyperplane defined by $\\mathbf{w}\\cdot\\mathbf{x}+b=0$ ***separates*** the classifications of data.\n",
    "However, this separation is not in general unique as we illustrate below.\n",
    "\n",
    "<img src=\"which_hyperplane_SVM.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SVM formulates attempts to solve this problem by re-formulating it as an optimization problem on the ***margin***. What is the margin? We illustrate the basic concept of margin below.\n",
    "\n",
    "<img src=\"maximizing_margin_SVM.png\" width=50%>\n",
    "\n",
    "Basically, we are trying to maximally separate the distinct classes in the data so that the classifier does a good job of avoiding misclassification on unseen data. \n",
    "The optimization problem is then stated as:\n",
    "> Minimize $\\sqrt{\\sum_{j=1}^m w_j^2}$ subject to the constraints that $y^{(i)}=\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b$ for $1\\leq i\\leq N$.\n",
    "\n",
    "Since minimizing $\\sum_{j=1}^m w_j^2$ will minimize $\\sqrt{\\sum_{j=1}^m w_j^2}$ (the square root is a monotonic increasing function), we instead formulate the optimization problem as:\n",
    "> Minimize $\\frac{1}{2}\\sum_{j=1}^m w_j^2$ subject to the constraints that $y^{(i)}=\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b$ for $1\\leq i\\leq N$.\n",
    "\n",
    "Here, we multiply by $\\frac{1}{2}$ for convenience just as with the cost function seen in the previous lecture. Basically, we now have a (constrained) *quadratic optimization* problem. There are codes to solve these (which is great because we certainly don't want to do that ourselves). \n",
    "\n",
    "While this will in general do a better job than the perceptron in avoiding \"overfitting\" the data (which we may define here as creating the hyperplane $\\mathbf{w}\\cdot\\mathbf{x}+b=0$ that is *too close* to one of the classes of data), it still suffers from the same basic issue of requiring the data to be linearly separable. In practice, this is going to eliminate a lot of data sets that are *almost* linearly separable. This seems rather unfortunate considering the ADALINE was also based on a (unconstrained) quadratic optimization problem that could be applied to any data set. \n",
    "\n",
    "Well, in practice we introduce a *slack variable* that *softens* the margin (i.e., it weakens the constraints that each datum be properly classified). This is done as follows:\n",
    "> For $1\\leq i\\leq N$, let $\\xi^{(i)}=\\max\\{0, 1-y^{(i)}(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}-b)\\}$, which is the smallest non-negative number satisfying $y^{(i)}(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}-b)\\geq 1-\\xi^{(i)}$. Subject to these constraints, minimize the cost function $\\left(\\frac{1}{2}\\sum_{j=1}^m w_j^2\\right) + C\\sum_i^N \\xi^{(i)}$ where $C>0$ is a control how much we penalize a misclassification error. \n",
    "\n",
    "The variable $C>0$ is a new *hyperparameter* that is effectively a *regularization* used to control overfitting the data. \n",
    "\n",
    "Let's examine the impact of this hyperparameter in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_SVM_variable_C(x, y,  C):\n",
    "    clf = SVC(kernel=\"linear\", C=C)\n",
    "    \n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    clf.fit(x, y) # Learn the weights and bias\n",
    "    \n",
    "    x1_plot = np.linspace(np.min(x[:,0])-0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          np.max(x[:,0])+0.1*(np.max(x[:,0])-np.min(x[:,0])),\n",
    "                          51)\n",
    "    x2_plot = np.linspace(np.min(x[:,1])-0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          np.max(x[:,1])+0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          51)\n",
    "    x1_plot, x2_plot = np.meshgrid(x1_plot, x2_plot)\n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    Z = clf.decision_function(np.c_[x1_plot.ravel(), x2_plot.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "    Z = Z.reshape(x1_plot.shape)\n",
    "    ax.contourf(x1_plot, x2_plot, Z, cmap=cm, alpha=.8, levels=50)\n",
    "\n",
    "    ax.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "\n",
    "    ax.set_xlim(x1_plot.min(), x1_plot.max())\n",
    "    ax.set_ylim(x2_plot.min(), x2_plot.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "    score = clf.score(x, y)\n",
    "    title_str = 'SVM score = {:.2f}%'.format(score*100)\n",
    "    ax.set_title(title_str, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "num_training = 100\n",
    "bbox_1 = np.array([[0.5, 0], [1.5, 4]]) #first row is lower-left point and second row is upper-right point of class -1\n",
    "bbox_2 = np.array([[2, 2], [4, 4]]) # \" \" of class 1\n",
    "# Will assume equal probability of training data being in either box\n",
    "pseudo_data = np.random.uniform(low=0, high=1, size=num_training)\n",
    "x_data = np.zeros((num_training,2))\n",
    "y_data = -np.ones(num_training)\n",
    "for i in range(num_training):\n",
    "    if pseudo_data[i]<0.5:\n",
    "        x_data[i,0] = np.random.uniform(low=bbox_1[0,0], high=bbox_1[1,0])\n",
    "        x_data[i,1] = np.random.uniform(low=bbox_1[0,1], high=bbox_1[1,1])\n",
    "    else:\n",
    "        x_data[i,0] = np.random.uniform(low=bbox_2[0,0], high=bbox_2[1,0])\n",
    "        x_data[i,1] = np.random.uniform(low=bbox_2[0,1], high=bbox_2[1,1])\n",
    "        y_data[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact(visualize_SVM_variable_C, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         C = widgets.FloatText(value=0.025, step=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-examine the basketball data from the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Off_Eff = []\n",
    "Def_Eff = []\n",
    "\n",
    "y = []\n",
    "\n",
    "iter = 0\n",
    "for year in range(2009, 2020):\n",
    "    url = 'http://www.espn.com/nba/hollinger/teamstats/_/year/' + str(year)\n",
    "    df_regular_season = pd.read_html(url)[0]\n",
    "    Off_Eff.append(df_regular_season.loc[2:, 10].values)\n",
    "    Def_Eff.append(df_regular_season.loc[2:, 11].values)\n",
    "    \n",
    "    N = len(df_regular_season.loc[2:, 1].values)\n",
    "    \n",
    "    y.append(-np.ones(N))\n",
    "    url += '/seasontype/3'\n",
    "    df_playoffs = pd.read_html(url)[0]\n",
    "    \n",
    "    playoff_teams = df_playoffs.loc[2:,1].values\n",
    "    all_teams = df_regular_season.loc[2:,1].values\n",
    "    for i in range(16):\n",
    "        for j in range(N):\n",
    "            if playoff_teams[i] == all_teams[j]:\n",
    "                y[iter][j] = 1\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Off_Eff_All = np.concatenate(Off_Eff).astype('float')\n",
    "Def_Eff_All = np.concatenate(Def_Eff).astype('float')\n",
    "\n",
    "bball_features = np.vstack((Off_Eff_All.flatten(), Def_Eff_All.flatten())).T\n",
    "\n",
    "playoffs_All = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact(visualize_SVM_variable_C, \n",
    "         x = widgets.fixed(bball_features),\n",
    "         y = widgets.fixed(playoffs_All),\n",
    "         C = widgets.FloatText(value=0.025, step=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part(b)(iii): The kernel trick for nonlinear problems <a id='kernel-trick'>\n",
    "---\n",
    "\n",
    "<span style='background:rgba(255,255,0, 0.25); color:black'> Run the code cell below and click the \"play\" button to see the third recorded lecture associated with this notebook.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Running this cell with embed the short recorded lecture associated with this part of the notebook\n",
    "# 2. Press on the \"play\" button to start the video.\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('Still-need-to-create', width=800, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this to create visualizations in this part of the notebook\n",
    "def visualize_SVM_variable_kernel(x, y, ker='linear', C=1e-2):\n",
    "    clf = SVC(kernel=ker, C=C)\n",
    "    \n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    clf.fit(x, y) # Learn the weights and bias\n",
    "    \n",
    "    x1_plot = np.linspace(np.min(x[:,0])-0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          np.max(x[:,0])+0.1*(np.max(x[:,0])-np.min(x[:,0])),\n",
    "                          51)\n",
    "    x2_plot = np.linspace(np.min(x[:,1])-0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          np.max(x[:,1])+0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          51)\n",
    "    x1_plot, x2_plot = np.meshgrid(x1_plot, x2_plot)\n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    Z = clf.decision_function(np.c_[x1_plot.ravel(), x2_plot.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "    Z = Z.reshape(x1_plot.shape)\n",
    "    ax.contourf(x1_plot, x2_plot, Z, cmap=cm, alpha=.8, levels=50)\n",
    "\n",
    "    ax.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "\n",
    "    ax.set_xlim(x1_plot.min(), x1_plot.max())\n",
    "    ax.set_ylim(x2_plot.min(), x2_plot.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "    score = clf.score(x, y)\n",
    "    title_str = 'SVM score = {:.2f}%'.format(score*100)\n",
    "    ax.set_title(title_str, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Feature space mappings:*** <span>\n",
    "\n",
    "- Suppose there exists a \"feature mapping\" function $\\phi(\\cdot)$ that maps the features $\\mathbf{x}$ into a new *higher-dimensional* feature space, with feature vectors now denoted by $\\mathbf{z}$, so that the associated classifications defined by $\\mathbf{y}$ are \"more clearly separated.\" In other words, we transform the data $\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^N$ into the form $\\{(\\mathbf{z}^{(i)},y^{(i)})\\}_{i=1}^N$. If $m$ denotes the dimension of the original feature space, then let $m+f$ denote the dimension of the new feature space where $f$ denotes the additional \"fake\" features defined by the mapping\n",
    "<br><br>\n",
    "- The classifier is then constructed on the data set $\\{(\\mathbf{z}^{(i)},y^{(i)})\\}_{i=1}^N$. \n",
    "<br><br>\n",
    "- What are the disadvantages of feature mapping?\n",
    "<br><br>Storing higher-dimensional feature vectors $\\mathbf{z}$ requires more memory. Also, the quadratic programming task required to train an SVM involves inner products of the feature space, which is now computationally more expensive. \n",
    "<br><br> \n",
    "\n",
    "We show an example of a feature mapping below (without dealing with the SVM part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data that is not even \"almost\" linearly separable\n",
    "X_circle_data, Y_circle_data = make_circles(noise=0.1, factor=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the separable data. While there is no good \"dividing line\" \n",
    "# we can certainly see that there is a curve (in this case a circle) that\n",
    "# separates the classes.\n",
    "plt.figure()\n",
    "plt.scatter(X_circle_data[:,0], X_circle_data[:,1], c=Y_circle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feature mapping that utilizes the knowledge\n",
    "# that there is a \"hidden\" circular feature in the data\n",
    "def phi_4_circle_data(X):\n",
    "    n = len(X)\n",
    "    Z = np.zeros((n,3))\n",
    "    Z[:,0:2] = X\n",
    "    Z[:,2] = X[:,0]**2 + X[:,1]**2 # This is the square of the radii of circles passing through the feature data\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the 2-D features into a new 3-D feature space\n",
    "Z = phi_4_circle_data(X_circle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the separability in this higher-dimensional feature space\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(Z[:,0], Z[:,1], Z[:,2], c=Y_circle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***The kernel trick:*** <span>\n",
    "\n",
    "- The \"kernel trick\" is that instead of computing $\\mathbf{z}^{(i)}=\\phi(\\mathbf{x}^{(i)})$ for each datum $\\mathbf{x}^{(i)}$, we define a kernel function, $k(\\cdot,\\cdot)$ such that $k(\\mathbf{x}^{(i)},\\mathbf{x}^{(j)}) = \\phi(\\mathbf{x}^{(i)})^\\top\\phi(\\mathbf{x}^{(j)})$.\n",
    "<br><br>\n",
    "    In other words, we work directly on the original feature space when training the classifier so that we do not have to explicitly create and store the higher-dimensional features $\\mathbf{z}^{(i)}$ for each $1\\leq i\\leq N$ in memory. We just simply compute the inner products required in the training step directly.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "We show this below for the circle data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry how I came up with this\n",
    "def circle_kernel(x_i, x_j):\n",
    "    k = np.dot(x_i,x_j.T) + np.tensordot(np.linalg.norm(x_i,ord=2,axis=1),np.linalg.norm(x_j,ord=2,axis=1),axes=0)**2\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_SVM_variable_kernel(X_circle_data, Y_circle_data, ker=circle_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:rgba(255,0,255, 0.25); color:black'> ***Interpreting and using different kernels:*** <span>\n",
    "\n",
    "- A kernel may also be referred to as a *similarity function* that describes how similar two samples are to each other. In fact, this is related to the idea of radial basis functions (RBFs) from a module 03 assignment. \n",
    "<br><br>\n",
    "- The \"default\" choice for a kernel is often the Gaussian RBF, which is often referred to simply as \"the RBF kernel\" and usually passed to the classifer as the string `'rbf'`. \n",
    "<br><br>\n",
    "    - The Gaussian RBF introduces another hyperparameter, which we will not worry about here.\n",
    "    <br><br>\n",
    "- Other common kernel options are `'linear'`, `'poly'`, and `'sigmoid'` (read more about these in the documentation of SVM here: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "<br><br>\n",
    "    - As with the Gaussian RBF, the `'poly'` and `'sigmoid'` introduce another hyperparameter. We again do not worry about that here.\n",
    "\n",
    "Generally, people first try out these common \"built-in\" kernels before trying to do anything that is user-defined like we did above. \n",
    "\n",
    "Let's try different kernels on the circle data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_SVM_variable_kernel, \n",
    "         x = widgets.fixed(X_circle_data),\n",
    "         y = widgets.fixed(Y_circle_data),\n",
    "         ker = widgets.Text('rbf'),\n",
    "         C = widgets.FloatText(value=0.025, step=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try this out on the basketball data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact_manual(visualize_SVM_variable_kernel, \n",
    "         x = widgets.fixed(bball_features),\n",
    "         y = widgets.fixed(playoffs_All),\n",
    "         ker = widgets.Text('rbf'),\n",
    "         C = widgets.FloatText(value=0.025, step=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n",
    "\n",
    "## <span style='background:rgba(0,255,255, 0.5); color:black'>Activity: Summary</span> <a id='activity-summary'/></a>\n",
    "\n",
    "Summarize some of the key takeaways/points from this notebook in a list below and prepare a few code examples related to these takeaways/points in the code cells below. You need to have at least one example for each of your summary points and you need at least three summary points.\n",
    "\n",
    "In this notebook, we have seen the following:\n",
    "\n",
    "- [Your summary point 1 goes here]\n",
    "\n",
    "\n",
    "- [Your summary point 2 goes here]\n",
    "\n",
    "\n",
    "- [Your summary point 3 goes here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href='#Contents'>Click here to return to Notebook Contents</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
