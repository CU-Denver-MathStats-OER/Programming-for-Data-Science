{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "28864eed-7d28-4abb-b7bb-24fa47287cad",
      "metadata": {
        "id": "28864eed-7d28-4abb-b7bb-24fa47287cad"
      },
      "source": [
        "# Math  1376: Programming for Data Science\n",
        "---\n",
        "\n",
        "## External activities for 04-Computational-Applications-part-a\n",
        "---\n",
        "\n",
        "**Expected time to completion: 2-3 hours**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "YouTubeVideo('SHgDH9La6fE', width=800, height=450)"
      ],
      "metadata": {
        "id": "jYTGXotEZwAB"
      },
      "id": "jYTGXotEZwAB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive within colab, which is necessary to import a module\n",
        "# stored in your Drive. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "npcIDu9pZnD4"
      },
      "id": "npcIDu9pZnD4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert the directory\n",
        "import sys\n",
        "\n",
        "# This next line may be unique to you (watch the video)\n",
        "module_path = '/content/drive/MyDrive/Colab Notebooks/Programming-for-Data-Science/04-Computational-Applications/activities'\n",
        "\n",
        "sys.path.insert(0,module_path)"
      ],
      "metadata": {
        "id": "kOVFXC0nZoUE"
      },
      "id": "kOVFXC0nZoUE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import MyFunctionClasses as MyFC\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iaH-XQ_OaA0O"
      },
      "id": "iaH-XQ_OaA0O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "678f9bbf-fc38-4b3e-aff6-ed592922a0c6",
      "metadata": {
        "id": "678f9bbf-fc38-4b3e-aff6-ed592922a0c6"
      },
      "source": [
        "---\n",
        "\n",
        "## <mark>Activity 1: A bisection variant (https://en.wikipedia.org/wiki/Regula_falsi)</mark>\n",
        "\n",
        "The basic idea of *regula falsi* (i.e., the false position method) is quite simple. It follows 3 steps starting with an initial guess of an interval $[a,b]$ that may contain a root. \n",
        "\n",
        "1. Make a straight line between $(a,f(a))$ and $(b,f(b))$. \n",
        "\n",
        "2. Compute the $x$-intercept of this line and call that point $c$. \n",
        "\n",
        "3. Compute $f(c)$ and compare the sign to $f(a)$ (or $f(b)$) and update the interval to either $[a,c]$ or $[c,b]$ depending on the result of this comparison.\n",
        "\n",
        "Thus, this is just a variant of the bisection algorithm. The fundamental difference is that instead of using the midpoint of the interval $[a,b]$ as the guess for the root $c$, we use *interpolation* to approximate the function with a simple function (a line) for which it is easy to determine the root. The exact root of this approximating function is then an approximate root of the exact function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0092cc93-8a26-4b6c-9bfe-e6b9d2e5cae8",
      "metadata": {
        "id": "0092cc93-8a26-4b6c-9bfe-e6b9d2e5cae8"
      },
      "source": [
        "**The same notes as the Activity 1 in the part a lecture notebook also apply here.**\n",
        "\n",
        "- Follow the steps below to sub-class from the `MyFunctionsRule` class (which is imported from the `MyFunctionClasses` module) so that this sub-class has a `compute_false_position` method attribute.\n",
        "    \n",
        "Step 1: Look at what is currently in the `MyFunctionsAintFalse` sub-class.\n",
        "    \n",
        "Step 2: Complete the method attribute `compute_false_position`. **Make sure that the new `compute_false_position` method attribute tests that the conditions are appropriate for running this algorithm.**  *Hint: All of this can be done with a copy/paste and edit of a single line of code. Yes, really! The false position algorithm differs from the bisection algorithm minimally. If you copy/paste the `compute_bisection` method from `MyFunctionsEnhanced` (which is a \"grandparent\" to the `MyFunctionsRule` class) into this method, then you only need to edit a single line of code in that method involved with computing `c` (the wiki article tells you exactly what the value of `c` should be in the false position algorithm). \n",
        "        \n",
        "Step 3: Make sure the code comments in the `compute_false_position` method are referring to it and not the bisection algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyFunctionsAintFalse(MyFC.MyFunctionsRule):\n",
        "    def __init__(self, f, a=None, b=None, n=None):\n",
        "        super().__init__(f=f, a=a, b=b, n=n)\n",
        "        return\n",
        "\n",
        "    def set_false_position_parameters(self, a=None, b=None, n=None, \n",
        "                                 tol_interval=None, tol_f=None):\n",
        "        '''\n",
        "        Useful for setting any or all of the parameters in the false position\n",
        "        algorithm by simply referring to the bisection parameters, which are \n",
        "        identical.\n",
        "        '''      \n",
        "        self.set_bisection_parameters(a, b, n, tol_interval, tol_f)           \n",
        "        return\n",
        "\n",
        "    def compute_false_position(self):\n",
        "        '''\n",
        "        STUDENTS NEED TO DO THIS.\n",
        "        '''"
      ],
      "metadata": {
        "id": "zVt2BsyPa67e"
      },
      "id": "zVt2BsyPa67e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69ce4f7f-3e76-4b56-a9b7-efcad9713369",
      "metadata": {
        "id": "69ce4f7f-3e76-4b56-a9b7-efcad9713369"
      },
      "outputs": [],
      "source": [
        "# Instructor created code cell\n",
        "\n",
        "f_ing_fantastic = MyFunctionsAintFalse(lambda x: np.sin(x**3 - x - 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6cb59c3-9c39-4f36-acd8-f6362d44a3fe",
      "metadata": {
        "id": "f6cb59c3-9c39-4f36-acd8-f6362d44a3fe"
      },
      "outputs": [],
      "source": [
        "f_ing_fantastic.plot(-1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13245f04-3ee2-4808-af97-d88c1801cf90",
      "metadata": {
        "id": "13245f04-3ee2-4808-af97-d88c1801cf90"
      },
      "outputs": [],
      "source": [
        "# Instructor created code cell to compare the methods\n",
        "\n",
        "f_ing_fantastic.set_false_position_parameters(a=-1, b=1.75, n=2)\n",
        "f_ing_fantastic.compute_bisection()\n",
        "f_ing_fantastic.plot(-1, 2, show_root=True)\n",
        "\n",
        "f_ing_fantastic.compute_false_position()\n",
        "f_ing_fantastic.plot(-1, 2, show_root=True)  # c = 0.932 if you did this right"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructor created code cell\n",
        "\n",
        "f_ing_fantastic.set_false_position_parameters(a=-1, b=1.75, n=5)\n",
        "f_ing_fantastic.compute_bisection()\n",
        "f_ing_fantastic.plot(-1, 2, show_root=True)\n",
        "\n",
        "f_ing_fantastic.compute_false_position()\n",
        "f_ing_fantastic.plot(-1, 2, show_root=True)  # c = 1.52 if you did this right"
      ],
      "metadata": {
        "id": "dqD-RqykeCll"
      },
      "id": "dqD-RqykeCll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "caf7c898-c0ef-440d-8779-84681c8639bd",
      "metadata": {
        "id": "caf7c898-c0ef-440d-8779-84681c8639bd"
      },
      "source": [
        "In computational and data science, we often must *choose* between implementing competing algorithms that solve the same class of problem in different ways. An important lesson is that if two or more algorithms exist to solve a problem, then there will be cases where one is preferable to the other. The question then becomes, which should you apply for a particular problem?   \n",
        "    \n",
        "- Use a mixture of markdown and code cells below to compare the bisection algorithm and false position method. \n",
        "\n",
        "  You may find the wiki articles useful to reference here as well as doing some searching on Google (or your favorite search engine). \n",
        "\n",
        "  Describe and implement problems where one method seemingly does better than another (e.g., what happens if the function is approximately linear near its root? what happens if the function is very \"flat\" near its root?)? Can you explain why that is happening? When would you consider using one method over another? What questions about these methods and comparing them seem worth exploring to you? This is intentionally left as open as possible for you to pursue what you think are intriguing questions to pose, research, and answer.   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put an example here where bisection outperforms false position"
      ],
      "metadata": {
        "id": "2HiJYz48gpHJ"
      },
      "id": "2HiJYz48gpHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put an example here where false position outperforms bisection"
      ],
      "metadata": {
        "id": "b2Sc5HPGgp24"
      },
      "id": "b2Sc5HPGgp24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KHULo_kGgqRe"
      },
      "id": "KHULo_kGgqRe"
    },
    {
      "cell_type": "markdown",
      "id": "90624636-bc89-40a8-914d-d8916d7e969b",
      "metadata": {
        "id": "90624636-bc89-40a8-914d-d8916d7e969b"
      },
      "source": [
        "End of Activity 1.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## <mark> Activity 2: [Monte Carlo (MC) Integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration) </mark>\n",
        "\n",
        "\n",
        "The basic idea of MC integration is to estimate $\\int_A f(x)$ using the *average* of a random sample of $f(x)$ values. \n",
        "\n",
        "The question is this: how do we generate a random sample of $f(x)$ values to do this?\n",
        "\n",
        "Well, we generate a random sample of $x$ values in the set $A$, and then evaluate the function $f$ at these random inputs. Then, we simply compute the sample average of $A$. \n",
        "\n",
        "Are there any sticky points to this?\n",
        "\n",
        "Well, a few simple examples will illustrate just about all you possibly need to know.\n",
        "\n",
        "Suppose $f(x)=5$ on $[0,1]$. From the lecture, we know that $\\int_{[0,1]} 5 = 5(1-0)=5$. So, this is what our MC estimate should be approximating.\n",
        "\n",
        "In fact, for any random sample of $x$-values between $[0,1]$, we will *always* get that the corresponding function values are *all* 5 because $f(x)$ is a constant 5 on this interval. So, what would the sample average be of these randomly sampled function values? Well, 5 of course! Whoa! The MC estimate is *exact* in this case. Great!\n",
        "\n",
        "What if $f(x)=5$ on $[0,2]$? So, all we have done is change the set from $[0,1]$ to $[0,2]$. We know the exact answer should now be $\\int_{[0,2]} 5 = 5(2-0)=10$. However, by the same reasoning, any random sample of $f(x)$ values will produce a sample average of $5$ not $10$. \n",
        "\n",
        "What if $f(x)=5$ on $[0,0.5]$? Again, the exact answer is $\\int_{[0,0.5]} 5 = 5(0.5-0)=2.5$, yet any sample average of $f(x)$ values will produce $5$ not $2.5$. \n",
        "\n",
        "So, what is going on? Well, we need to *weight* the sample average by the *size* of the set $A$. \n",
        "\n",
        "In summary, if $\\{x_i\\}_{i=1}^N \\subset A$ is a *uniform* random sample from the set $A$, and we let $\\mu(A)$ denote the *measure* (i.e., size) of $A$, then an MC estimate of the integral is given by\n",
        "$$\n",
        "    \\int_A f(x) \\approx \\mu(A)\\frac{1}{N}\\sum_{i=1}^N f(x_i).\n",
        "$$\n",
        "\n",
        "Let's play with this below and discuss what we are seeing."
      ],
      "metadata": {
        "id": "aeN7srULhOG9"
      },
      "id": "aeN7srULhOG9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89cebb15-d3ec-4c1f-bdf6-d1fe37c32bf4",
      "metadata": {
        "id": "89cebb15-d3ec-4c1f-bdf6-d1fe37c32bf4"
      },
      "outputs": [],
      "source": [
        "def compute_1D_MC_approx(f, a, b, n):\n",
        "    x_random = np.random.uniform(low=a, high=b, size=int(n))\n",
        "    mu_A = b-a  # mu_A=measure (size) of A\n",
        "    avg_func = np.mean(f(x_random))\n",
        "    MC_est = mu_A * avg_func\n",
        "    return (avg_func, MC_est)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaYWa4lQhCAo"
      },
      "source": [
        "***Let's do some numerical experiments and analysis.***\n",
        "\n",
        "Below, we show how to\n",
        "\n",
        "1. Use the `compute_1D_MC_approx` function defined above and understand its variability due to random sampling.\n",
        "\n",
        "2. Analyze the rate of convergence (ROC).\n",
        "\n",
        "3. Create interactive visualizations of the `compute_1D_MC_approx` function that help us tell a story."
      ],
      "id": "eaYWa4lQhCAo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbAmhtTqhCAo"
      },
      "outputs": [],
      "source": [
        "f = lambda x: (x + 1) * (x - 5) * np.sin(3*x) "
      ],
      "id": "xbAmhtTqhCAo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJp7gnAIhCAp"
      },
      "outputs": [],
      "source": [
        "# These all look the same, but are different, why?\n",
        "int_approx  = compute_1D_MC_approx(f, a=2, b=4, n=100)[1]\n",
        "print(int_approx)\n",
        "int_approx  = compute_1D_MC_approx(f, a=2, b=4, n=100)[1]\n",
        "print(int_approx)\n",
        "int_approx  = compute_1D_MC_approx(f, a=2, b=4, n=100)[1]\n",
        "print(int_approx)"
      ],
      "id": "iJp7gnAIhCAp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duooc6EBhCAp"
      },
      "source": [
        "What we see above is an issue involving the random sampling of points between $[a,b]$ to generate an approximation of the average value of the function. As you can see by re-running the above code cell multiple times, there can be quite a bit of variability. Below, we compute 1000 approximations and visualize the results with a histogram."
      ],
      "id": "Duooc6EBhCAp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yOIV9J3hCAp"
      },
      "outputs": [],
      "source": [
        "num_trials = 1000\n",
        "int_approx = np.zeros(num_trials)\n",
        "for i in range(num_trials):\n",
        "    int_approx[i] = compute_1D_MC_approx(f, a=2, b=4, n=100)[1]"
      ],
      "id": "9yOIV9J3hCAp"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon \n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.collections import PatchCollection\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "from scipy.integrate import quad"
      ],
      "metadata": {
        "id": "ZFu7PO-8iK_r"
      },
      "id": "ZFu7PO-8iK_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su8KfvQXhCAq"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.hist(int_approx)\n",
        "plt.axvline(quad(f, a=2, b=4)[0], color='k')  # Plot a vertical line where the \"exact\" value of the integral is"
      ],
      "id": "Su8KfvQXhCAq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JIFAMp_hCAq"
      },
      "source": [
        "Looking at the histogram above, we see that the histogram appears to be centered around the exact value, so we compute the mean (sample average) of the estimates below."
      ],
      "id": "0JIFAMp_hCAq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8ljfvgQhCAq"
      },
      "outputs": [],
      "source": [
        "print(int_approx.mean())\n",
        "print(quad(f, a=2, b=4)[0])"
      ],
      "id": "q8ljfvgQhCAq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfgFlgMohCAq"
      },
      "source": [
        "So, while any single MC estimate may be \"bad\", the expected value (i.e., the mean) of all MC estimates is quite good. In fact, the MC method produces what is known as an *unbiased* estimator of the integral meaning that its expected value is exactly the integral we want. Unfortunately, the variance in this estimator (i.e., how much any single MC estimate may vary around the estimate) can be quite large. How do we reduce the variance? We increase the number of points used to estimate the average value of the function. We show this below."
      ],
      "id": "zfgFlgMohCAq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl6AFqRshCAr"
      },
      "outputs": [],
      "source": [
        "# This can take a few seconds to run\n",
        "# Students should comment each line of code here and explain the shape of int_approx and what \n",
        "# is stored in the ij component of this array.\n",
        "num_trials = 1000\n",
        "ns = np.logspace(2, 4, num=3).astype(int)\n",
        "int_approx = np.zeros((3,num_trials))\n",
        "for i in range(3):\n",
        "    for j in range(num_trials):\n",
        "        int_approx[i,j] = compute_1D_MC_approx(f, a=2, b=4, n=ns[i])[1]"
      ],
      "id": "fl6AFqRshCAr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15AAUGwEhCAr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "for i in range(3):\n",
        "    plt.hist(int_approx[i,:], alpha=1-0.2*i, label='n=' + str(ns[i]))\n",
        "plt.legend(fontsize=12)\n",
        "plt.axvline(quad(f, a=2, b=4)[0], color='k') # Plot a vertical line where the \"exact\" value of the integral is"
      ],
      "id": "15AAUGwEhCAr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCV-0zeihCAr"
      },
      "source": [
        "In the histograms above, we see that increasing `n` will reduce the variance in the MC estimates. In other words,  each individual MC estimate is more likely to be closer to its mean value. Since the mean values are all the same (the MC estimates are unbiased), this means that a larger `n` value is more likely to produce *accurate* estimates of the integral. \n",
        "\n",
        "It is more common to use the standard deviation to quantify variability in estimates around the mean value than the variance because it is in the same units as the mean value. The standard deviation is simply the square root of the variance. It can be computed using a built-in function within numpy as we show below where we analyze the rate of convergence (ROC) of MC estimates in terms of the standard deviation."
      ],
      "id": "JCV-0zeihCAr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6VTyMishCAs"
      },
      "outputs": [],
      "source": [
        "# Now visualize the convergence of the left-hand rule\n",
        "plt.figure()\n",
        "plt.loglog(ns, int_approx.std(axis=1))\n",
        "plt.xlabel('# of random samples', fontsize=12)\n",
        "plt.title('St.Dev. of MC Estimates vs. # of samples')"
      ],
      "id": "a6VTyMishCAs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnknoUQNhCAs"
      },
      "outputs": [],
      "source": [
        "ROC_estimate = np.polyfit(np.log(ns), np.log(int_approx.std(axis=1)), 1)[0]\n",
        "\n",
        "print(ROC_estimate) "
      ],
      "id": "OnknoUQNhCAs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPbAhB-thCAs"
      },
      "source": [
        "The above value is approximately -0.5, which is the theoretical rate guaranteed by the Central Limit Theorem (you should really take some probability theory and statistics classes is the lesson here). What this means is that for each order of magnitude increase in the number of samples, the standard deviation is decreased by *half* an order of magnitude.\n",
        "\n",
        "Is this worse than the left-, right-, or mid-point rules for the deterministic methods? Yes, yes it is. Except there is a huge caveat here. The geometric methods for approximating integrals only work in extremely low dimensions. Geometric or other deterministic \"quadrature\" or \"cubature\" methods for approximating integrals suffer from what is known as the curse of dimensionality (look this up). We will not dwell on this here other than to say that as the dimension goes way up, the quality of the deterministic estimates goes way down unless an exponentially increasing number of points are used to estimate the integral.\n",
        "\n",
        "The rate of convergence for the MC estimate is *independent* of dimension (although this does not quite tell the whole story, it is a big reason why random or pseudo-random approaches to estimating integrals are preferred in high-dimensions). \n",
        "\n",
        "Below, we create some visualizations to help capture the story above."
      ],
      "id": "QPbAhB-thCAs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLap_T8bhCAt"
      },
      "outputs": [],
      "source": [
        "def plot_MC_1D(f, a, b, n, x_min, x_max):\n",
        "    avg_func, MC_est = compute_1D_MC_approx(f, a, b, n)\n",
        "        \n",
        "    ####### Everything below is for plotting\n",
        "    x = np.linspace(x_min, x_max, 101)\n",
        "    y = f(x)\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, y, 'r', linewidth=2)\n",
        "\n",
        "    plt.axhline(0, linewidth=1, linestyle=':', c='k') #plot typical x-axis\n",
        "    \n",
        "    # Make the shaded region\n",
        "    ix = np.linspace(a, b, 101)\n",
        "    iy = f(ix)\n",
        "    verts = [(a, 0), *zip(ix, iy), (b, 0)]\n",
        "    poly = Polygon(verts, facecolor='0.9', edgecolor='0.5')\n",
        "    ax.add_patch(poly)\n",
        "    \n",
        "    # Make an \"average\" rectangle corresponding to the MC_est\n",
        "    verts = [(a, 0), (a, avg_func), (b, avg_func) , (b, 0)]\n",
        "    rect = Polygon(verts, facecolor='b', alpha=0.25, edgecolor='0.5')\n",
        "    ax.add_patch(rect)\n",
        "    \n",
        "    ax.set_title(r\"$\\int_a^b f(x)\\mathrm{d}x \\approx \\frac{b-a}{N}\\sum_{i=1}^N f(x_i) =$ %3.2f\" %MC_est, fontsize=20)\n",
        "\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.set_xticks((a, b))\n",
        "    ax.set_xticklabels(('$a$', '$b$'))\n",
        "    plt.show()"
      ],
      "id": "yLap_T8bhCAt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma21A9sbhCAt"
      },
      "outputs": [],
      "source": [
        "%reset -f out\n",
        "\n",
        "# Using an interact_manual widget so that we can re-run easily to observe\n",
        "# the variations in the MC estimates due to random sampling.\n",
        "\n",
        "interact_manual(plot_MC_1D, \n",
        "         f = widgets.fixed(lambda x: (x + 1) * (x - 5) * np.sin(3*x)),\n",
        "         a = widgets.FloatSlider(value=1, min=-1, max=5, step=0.1),\n",
        "         b = widgets.FloatSlider(value=2.5, min=-1, max=5, step=0.1),\n",
        "         n = widgets.IntSlider(value=1E2, min=10, max=1E5, step=10),\n",
        "         x_min = widgets.fixed(-1),\n",
        "         x_max = widgets.fixed(5))"
      ],
      "id": "ma21A9sbhCAt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn7BLGRmhCAt"
      },
      "source": [
        "<mark> ***Student to-do's (finally, right?)*** </mark>\n",
        "\n",
        "Feel free to create many new code and markdown cells below as you work through this activity.\n",
        "\n",
        "- Complete the wrapper function, `multiple_1D_MC_approx`, in the code cell below except that we now assume `n` is a list of integers containing the different numbers of random points used to approximate the integral. The wrapper function also has an additional parameter `num_trials`, which is an `int` type that specifies the number of times (i.e., the number of trials) to repeat the approximations for each value in the list `n` values. The default `num_trials` is set to 10. \n",
        "    \n",
        "  The function should output a 2-dimensional numpy array of shape `(len(n),num_trials)` where the `[i,j]` component (in Python syntax) gives the (j+1)th MC approximation of the integral using `n[i]` random points.  "
      ],
      "id": "Cn7BLGRmhCAt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRvzhvZkhCAt"
      },
      "outputs": [],
      "source": [
        "def multiple_1D_MC_approx(f, a, b, n, num_trials=10):\n",
        "    \n",
        "    int_f_approx = np.zeros(( , )) #COMPLETE THIS PART TO INITIALIZE THE ARRAY TO ALL ZEROS\n",
        "        \n",
        "    for i in range(len(n)):\n",
        "        \n",
        "        for j in range(num_trials):\n",
        "            \n",
        "            int_f_approx[i,j] = compute_1D_MC_approx() #COMPLETE THIS PART\n",
        "            \n",
        "    return int_f_approx"
      ],
      "id": "HRvzhvZkhCAt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvDTuLeHhCAu"
      },
      "source": [
        "- For a number of different `lambda` functions including  `f = lambda x: x`, `f = lambda x: x**2`, and at least one of your own choosing, compute the errors for the MC approximations of the integrals with `n=[int(1E1), int(1E2), int(1E3), int(1E4)]`, `a=0`, and `b=1`. For each function, create a numpy array of shape `(len(n),num_trials)` to store the errors. "
      ],
      "id": "pvDTuLeHhCAu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI9GNEEKhCAu"
      },
      "outputs": [],
      "source": [],
      "id": "vI9GNEEKhCAu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H92E1zLXhCAu"
      },
      "outputs": [],
      "source": [],
      "id": "H92E1zLXhCAu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbHIycaWhCAu"
      },
      "outputs": [],
      "source": [],
      "id": "WbHIycaWhCAu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UklF5mNzhCAu"
      },
      "outputs": [],
      "source": [],
      "id": "UklF5mNzhCAu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSbuVuFHhCAv"
      },
      "outputs": [],
      "source": [],
      "id": "MSbuVuFHhCAv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJr4HlBahCAv"
      },
      "source": [
        "- For each of your different `lambda` functions, create two separate log-log plots with a legend that shows the *means* and *variances* (over the number of trials) of the computed errors vs. the number of random points used in the MC approximations. For the means plot, you should compute the absolute values of the mean errors (not the mean of the absolute value) in order to get the plot to show correctly."
      ],
      "id": "XJr4HlBahCAv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DpYlq5RhCAv"
      },
      "outputs": [],
      "source": [],
      "id": "5DpYlq5RhCAv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4h47WjvhCAv"
      },
      "outputs": [],
      "source": [],
      "id": "O4h47WjvhCAv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhKUagZchCAv"
      },
      "outputs": [],
      "source": [],
      "id": "dhKUagZchCAv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH5cFZblhCAw"
      },
      "outputs": [],
      "source": [],
      "id": "RH5cFZblhCAw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uJL3InihCAw"
      },
      "source": [
        "- Comment on your results in the Markdown cell below."
      ],
      "id": "0uJL3InihCAw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWzt99JvhCAw"
      },
      "source": [],
      "id": "xWzt99JvhCAw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "End of Activity 2.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-F39lrWnh2Jc"
      },
      "id": "-F39lrWnh2Jc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}